{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## La couche d'encodage est le label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_LABEL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    stddev = np.sqrt(1. / np.prod(shape[:-1]))\n",
    "    print(stddev)\n",
    "    initial = tf.random_normal(shape, stddev = stddev)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0., shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, NB_LABEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0353553390593\n"
     ]
    }
   ],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0178571428571\n"
     ]
    }
   ],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03125\n"
     ]
    }
   ],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_logit = tf.matmul(h_fc1, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tf.argmax(y_logit, 1)\n",
    "\n",
    "# multinomial softmax draw\n",
    "#indices = tf.multinomial(y_logit, 1)[:,0]\n",
    "\n",
    "depth = NB_LABEL\n",
    "y_hat = tf.one_hot(indices, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.316227766017\n"
     ]
    }
   ],
   "source": [
    "'''W_fc3 = weight_variable([10, 1024])\n",
    "b_fc3 = bias_variable([1024])\n",
    "\n",
    "#h_decoder = tf.nn.relu(tf.matmul(tf.nn.softmax(y_logit), W_fc3) + b_fc3)\n",
    "h_decoder = tf.nn.relu(tf.matmul(y_hat * y_logit, W_fc3) + b_fc3)\n",
    "#h_decoder = tf.matmul(y_hat, W_fc3) + b_fc3\n",
    "\n",
    "W_fc4 = weight_variable([1024, 784])\n",
    "b_fc4 = bias_variable([784])\n",
    "\n",
    "x_gen = tf.matmul(h_decoder, W_fc4) + b_fc4'''\n",
    "\n",
    "\n",
    "W_fc3 = weight_variable([10, 784])\n",
    "b_fc3 = bias_variable([784])\n",
    "#x_gen = tf.matmul(tf.nn.softmax(y_logit), W_fc3) + b_fc3\n",
    "x_gen = tf.matmul(y_hat, W_fc3) + b_fc3\n",
    "\n",
    "#x_gen = tf.matmul(y_hat * y_logit, W_fc3) + b_fc3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.cast(tf.equal(indices, tf.argmax(y_, 1)), tf.float32)\n",
    "reward = correct_prediction - (1 - correct_prediction)\n",
    "#reward_KL_plus = 1 #correct_prediction\n",
    "#reward_KL_moins = 1 - correct_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "p = tf.nn.softmax(y_logit)\n",
    "q = 1. / NB_LABEL\n",
    "\n",
    "#KL = tf.reduce_sum(p * y_logit,reduction_indices=1)\n",
    "KL_ref = tf.reduce_sum(p * tf.log(p/q), reduction_indices=1)\n",
    "KL = tf.reduce_mean(KL_ref) # + reward_KL_moins * (- tf.log(0.1) - KL_ref))\n",
    "#KL = tf.nn.softmax_cross_entropy_with_logits(labels=y_hat, logits=y_logit) #!! KL(1_\\hat{y})\n",
    "#KL = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_logit)) #!! KL(1_\\hat{y})\n",
    "\n",
    "#cross_entropy_loss = tf.reduce_mean(reward * tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_gen, labels=x), reduction_indices=1))\n",
    "#reward = 1\n",
    "cross_entropy_loss = tf.reduce_mean(reward  * tf.nn.softmax_cross_entropy_with_logits(labels=y_hat, logits=y_logit)) \n",
    "#cross_entropy_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_gen, labels=x))\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_gen, labels=x), reduction_indices = 1))\n",
    "#rewarded_loss = reward * (KL + cross_entropy_loss)\n",
    "\n",
    "rewarded_loss = KL +  cross_entropy_loss + reconstruction_loss\n",
    "                              \n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(rewarded_loss)\n",
    "accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.run(KL, feed_dict={x: batch[0], y_: batch[1]})\n",
    "#sess.run(reward, feed_dict={x: batch[0], y_: batch[1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.run(rewarded_cross_entropy, feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.14, consistency loss : 0.0187655, code_loss -1.46694, reconstr_loss 553.018\n",
      "step 100, training accuracy 0.86, consistency loss : 0.33715, code_loss 0.619071, reconstr_loss 549.864\n",
      "step 200, training accuracy 0.98, consistency loss : 0.348999, code_loss 0.951596, reconstr_loss 544.55\n",
      "step 300, training accuracy 0.98, consistency loss : 0.434332, code_loss 0.802675, reconstr_loss 539.839\n",
      "step 400, training accuracy 0.98, consistency loss : 0.456486, code_loss 0.763334, reconstr_loss 532.1\n",
      "step 500, training accuracy 0.94, consistency loss : 0.41078, code_loss 0.724965, reconstr_loss 528.127\n",
      "step 600, training accuracy 0.96, consistency loss : 0.419169, code_loss 0.755063, reconstr_loss 523.735\n",
      "step 700, training accuracy 0.98, consistency loss : 0.398874, code_loss 0.805686, reconstr_loss 518.131\n",
      "step 800, training accuracy 0.98, consistency loss : 0.408243, code_loss 0.811472, reconstr_loss 512.224\n",
      "step 900, training accuracy 0.94, consistency loss : 0.432623, code_loss 0.693004, reconstr_loss 508.836\n",
      "step 1000, training accuracy 1, consistency loss : 0.414182, code_loss 0.857667, reconstr_loss 503.221\n",
      "step 1100, training accuracy 0.98, consistency loss : 0.432117, code_loss 0.785539, reconstr_loss 498.695\n",
      "step 1200, training accuracy 0.98, consistency loss : 0.433343, code_loss 0.784676, reconstr_loss 491.789\n",
      "step 1300, training accuracy 0.96, consistency loss : 0.43963, code_loss 0.709856, reconstr_loss 488.377\n",
      "step 1400, training accuracy 0.98, consistency loss : 0.463355, code_loss 0.726107, reconstr_loss 483.534\n",
      "step 1500, training accuracy 1, consistency loss : 0.508953, code_loss 0.733857, reconstr_loss 479.577\n",
      "step 1600, training accuracy 1, consistency loss : 0.387111, code_loss 0.871356, reconstr_loss 474.992\n",
      "step 1700, training accuracy 1, consistency loss : 0.436106, code_loss 0.823124, reconstr_loss 470.765\n",
      "step 1800, training accuracy 0.98, consistency loss : 0.453091, code_loss 0.741161, reconstr_loss 465.587\n",
      "step 1900, training accuracy 1, consistency loss : 0.396053, code_loss 0.881224, reconstr_loss 461.509\n",
      "step 2000, training accuracy 1, consistency loss : 0.454, code_loss 0.789786, reconstr_loss 456.131\n",
      "step 2100, training accuracy 1, consistency loss : 0.443542, code_loss 0.807082, reconstr_loss 452.317\n",
      "step 2200, training accuracy 1, consistency loss : 0.452058, code_loss 0.779057, reconstr_loss 449.722\n",
      "step 2300, training accuracy 1, consistency loss : 0.442644, code_loss 0.780424, reconstr_loss 447.35\n",
      "step 2400, training accuracy 1, consistency loss : 0.465724, code_loss 0.759424, reconstr_loss 439.725\n",
      "step 2500, training accuracy 1, consistency loss : 0.455726, code_loss 0.781815, reconstr_loss 437.158\n",
      "step 2600, training accuracy 0.98, consistency loss : 0.418911, code_loss 0.766488, reconstr_loss 433.313\n",
      "step 2700, training accuracy 1, consistency loss : 0.424912, code_loss 0.815561, reconstr_loss 430.451\n",
      "step 2800, training accuracy 0.96, consistency loss : 0.46699, code_loss 0.675331, reconstr_loss 426.281\n",
      "step 2900, training accuracy 1, consistency loss : 0.478137, code_loss 0.743895, reconstr_loss 420.818\n",
      "step 3000, training accuracy 1, consistency loss : 0.447757, code_loss 0.783651, reconstr_loss 415.376\n",
      "step 3100, training accuracy 1, consistency loss : 0.450272, code_loss 0.776436, reconstr_loss 412.501\n",
      "step 3200, training accuracy 1, consistency loss : 0.429205, code_loss 0.798575, reconstr_loss 411.558\n",
      "step 3300, training accuracy 1, consistency loss : 0.472365, code_loss 0.750715, reconstr_loss 406.408\n",
      "step 3400, training accuracy 1, consistency loss : 0.459827, code_loss 0.757101, reconstr_loss 405.152\n",
      "step 3500, training accuracy 1, consistency loss : 0.470923, code_loss 0.751775, reconstr_loss 399.23\n",
      "step 3600, training accuracy 0.98, consistency loss : 0.50145, code_loss 0.667853, reconstr_loss 395.54\n",
      "step 3700, training accuracy 1, consistency loss : 0.449752, code_loss 0.776066, reconstr_loss 391.301\n",
      "step 3800, training accuracy 1, consistency loss : 0.431958, code_loss 0.794001, reconstr_loss 392.851\n",
      "step 3900, training accuracy 1, consistency loss : 0.439428, code_loss 0.784146, reconstr_loss 384.378\n",
      "step 4000, training accuracy 1, consistency loss : 0.445048, code_loss 0.779487, reconstr_loss 384.723\n",
      "step 4100, training accuracy 0.98, consistency loss : 0.450929, code_loss 0.709662, reconstr_loss 380.19\n",
      "step 4200, training accuracy 1, consistency loss : 0.487606, code_loss 0.737497, reconstr_loss 381.479\n",
      "step 4300, training accuracy 1, consistency loss : 0.448766, code_loss 0.773458, reconstr_loss 378.741\n",
      "step 4400, training accuracy 1, consistency loss : 0.450205, code_loss 0.769898, reconstr_loss 372.712\n",
      "step 4500, training accuracy 1, consistency loss : 0.463052, code_loss 0.761683, reconstr_loss 370.683\n",
      "step 4600, training accuracy 0.98, consistency loss : 0.41282, code_loss 0.757193, reconstr_loss 364.318\n",
      "step 4700, training accuracy 1, consistency loss : 0.479091, code_loss 0.734332, reconstr_loss 364.574\n",
      "step 4800, training accuracy 1, consistency loss : 0.461007, code_loss 0.762462, reconstr_loss 358.11\n",
      "step 4900, training accuracy 0.96, consistency loss : 0.463281, code_loss 0.675088, reconstr_loss 355.024\n",
      "step 5000, training accuracy 1, consistency loss : 0.467014, code_loss 0.754516, reconstr_loss 351.345\n",
      "step 5100, training accuracy 1, consistency loss : 0.482942, code_loss 0.73836, reconstr_loss 348.462\n",
      "step 5200, training accuracy 1, consistency loss : 0.444185, code_loss 0.771568, reconstr_loss 348.778\n",
      "step 5300, training accuracy 1, consistency loss : 0.455699, code_loss 0.760732, reconstr_loss 348.873\n",
      "step 5400, training accuracy 1, consistency loss : 0.444578, code_loss 0.780651, reconstr_loss 341.769\n",
      "step 5500, training accuracy 0.98, consistency loss : 0.451179, code_loss 0.716449, reconstr_loss 338.406\n",
      "step 5600, training accuracy 1, consistency loss : 0.482367, code_loss 0.737428, reconstr_loss 343.06\n",
      "step 5700, training accuracy 1, consistency loss : 0.45991, code_loss 0.75891, reconstr_loss 339.739\n",
      "step 5800, training accuracy 1, consistency loss : 0.444099, code_loss 0.781747, reconstr_loss 331.403\n",
      "step 5900, training accuracy 1, consistency loss : 0.464284, code_loss 0.754624, reconstr_loss 334.108\n",
      "step 6000, training accuracy 1, consistency loss : 0.487781, code_loss 0.730288, reconstr_loss 325.999\n",
      "step 6100, training accuracy 1, consistency loss : 0.448993, code_loss 0.776422, reconstr_loss 328.776\n",
      "step 6200, training accuracy 1, consistency loss : 0.433475, code_loss 0.797275, reconstr_loss 324.33\n",
      "step 6300, training accuracy 1, consistency loss : 0.473011, code_loss 0.74497, reconstr_loss 324.524\n",
      "step 6400, training accuracy 0.96, consistency loss : 0.451616, code_loss 0.658747, reconstr_loss 324.753\n",
      "step 6500, training accuracy 1, consistency loss : 0.441017, code_loss 0.78521, reconstr_loss 311.485\n",
      "step 6600, training accuracy 1, consistency loss : 0.47743, code_loss 0.734841, reconstr_loss 324.264\n",
      "step 6700, training accuracy 0.98, consistency loss : 0.462249, code_loss 0.704292, reconstr_loss 315.094\n",
      "step 6800, training accuracy 1, consistency loss : 0.445923, code_loss 0.772966, reconstr_loss 312.917\n",
      "step 6900, training accuracy 1, consistency loss : 0.444075, code_loss 0.77454, reconstr_loss 312.66\n",
      "step 7000, training accuracy 1, consistency loss : 0.471614, code_loss 0.743639, reconstr_loss 309.563\n",
      "step 7100, training accuracy 1, consistency loss : 0.464685, code_loss 0.757233, reconstr_loss 308.001\n",
      "step 7200, training accuracy 0.98, consistency loss : 0.438399, code_loss 0.728786, reconstr_loss 297.405\n",
      "step 7300, training accuracy 1, consistency loss : 0.507652, code_loss 0.716179, reconstr_loss 307.018\n",
      "step 7400, training accuracy 1, consistency loss : 0.449113, code_loss 0.766385, reconstr_loss 299.725\n",
      "step 7500, training accuracy 1, consistency loss : 0.455421, code_loss 0.762978, reconstr_loss 301.099\n",
      "step 7600, training accuracy 1, consistency loss : 0.491582, code_loss 0.720079, reconstr_loss 300.772\n",
      "step 7700, training accuracy 1, consistency loss : 0.474859, code_loss 0.742739, reconstr_loss 292.859\n",
      "step 7800, training accuracy 1, consistency loss : 0.484883, code_loss 0.726098, reconstr_loss 291.121\n",
      "step 7900, training accuracy 1, consistency loss : 0.451979, code_loss 0.762714, reconstr_loss 278.714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8000, training accuracy 1, consistency loss : 0.458819, code_loss 0.760843, reconstr_loss 297.064\n",
      "step 8100, training accuracy 1, consistency loss : 0.458472, code_loss 0.756359, reconstr_loss 295.449\n",
      "step 8200, training accuracy 0.98, consistency loss : 0.457381, code_loss 0.711196, reconstr_loss 276.964\n",
      "step 8300, training accuracy 1, consistency loss : 0.469771, code_loss 0.747119, reconstr_loss 282.967\n",
      "step 8400, training accuracy 1, consistency loss : 0.4375, code_loss 0.782222, reconstr_loss 273.692\n",
      "step 8500, training accuracy 1, consistency loss : 0.467055, code_loss 0.755592, reconstr_loss 267.828\n",
      "step 8600, training accuracy 1, consistency loss : 0.478861, code_loss 0.731618, reconstr_loss 276\n",
      "step 8700, training accuracy 1, consistency loss : 0.441532, code_loss 0.775604, reconstr_loss 273.202\n",
      "step 8800, training accuracy 1, consistency loss : 0.477175, code_loss 0.734638, reconstr_loss 277.613\n",
      "step 8900, training accuracy 1, consistency loss : 0.453935, code_loss 0.757993, reconstr_loss 268.524\n",
      "step 9000, training accuracy 1, consistency loss : 0.464595, code_loss 0.749561, reconstr_loss 273.822\n",
      "step 9100, training accuracy 1, consistency loss : 0.460078, code_loss 0.766883, reconstr_loss 271.137\n",
      "step 9200, training accuracy 1, consistency loss : 0.453275, code_loss 0.75716, reconstr_loss 269.869\n",
      "step 9300, training accuracy 1, consistency loss : 0.469915, code_loss 0.743482, reconstr_loss 265.581\n",
      "step 9400, training accuracy 1, consistency loss : 0.449467, code_loss 0.759265, reconstr_loss 266.334\n",
      "step 9500, training accuracy 1, consistency loss : 0.425597, code_loss 0.785271, reconstr_loss 258.852\n",
      "step 9600, training accuracy 1, consistency loss : 0.450511, code_loss 0.760361, reconstr_loss 264.444\n",
      "step 9700, training accuracy 1, consistency loss : 0.451369, code_loss 0.776054, reconstr_loss 249.007\n",
      "step 9800, training accuracy 1, consistency loss : 0.476285, code_loss 0.730767, reconstr_loss 254.798\n",
      "step 9900, training accuracy 0.98, consistency loss : 0.455752, code_loss 0.708769, reconstr_loss 263.917\n",
      "step 10000, training accuracy 1, consistency loss : 0.484804, code_loss 0.722775, reconstr_loss 258.646\n",
      "step 10100, training accuracy 1, consistency loss : 0.468565, code_loss 0.742774, reconstr_loss 255.036\n",
      "step 10200, training accuracy 1, consistency loss : 0.463203, code_loss 0.745018, reconstr_loss 250.985\n",
      "step 10300, training accuracy 1, consistency loss : 0.459913, code_loss 0.753868, reconstr_loss 246.244\n",
      "step 10400, training accuracy 1, consistency loss : 0.472994, code_loss 0.73986, reconstr_loss 247.799\n",
      "step 10500, training accuracy 1, consistency loss : 0.449401, code_loss 0.766413, reconstr_loss 254.618\n",
      "step 10600, training accuracy 1, consistency loss : 0.455773, code_loss 0.762466, reconstr_loss 241.392\n",
      "step 10700, training accuracy 1, consistency loss : 0.457904, code_loss 0.752823, reconstr_loss 245.269\n",
      "step 10800, training accuracy 1, consistency loss : 0.449185, code_loss 0.762422, reconstr_loss 255.871\n",
      "step 10900, training accuracy 1, consistency loss : 0.446089, code_loss 0.767582, reconstr_loss 240.924\n",
      "step 11000, training accuracy 1, consistency loss : 0.441531, code_loss 0.769626, reconstr_loss 241.22\n",
      "step 11100, training accuracy 1, consistency loss : 0.453877, code_loss 0.755506, reconstr_loss 244.103\n",
      "step 11200, training accuracy 1, consistency loss : 0.456387, code_loss 0.7512, reconstr_loss 247.03\n",
      "step 11300, training accuracy 1, consistency loss : 0.48601, code_loss 0.723923, reconstr_loss 237.669\n",
      "step 11400, training accuracy 1, consistency loss : 0.440633, code_loss 0.770813, reconstr_loss 240.294\n",
      "step 11500, training accuracy 1, consistency loss : 0.452446, code_loss 0.755622, reconstr_loss 244.635\n",
      "step 11600, training accuracy 1, consistency loss : 0.437247, code_loss 0.776799, reconstr_loss 229.63\n",
      "step 11700, training accuracy 1, consistency loss : 0.448922, code_loss 0.765282, reconstr_loss 240.164\n",
      "step 11800, training accuracy 0.98, consistency loss : 0.437638, code_loss 0.732244, reconstr_loss 234.4\n",
      "step 11900, training accuracy 1, consistency loss : 0.452892, code_loss 0.756758, reconstr_loss 232.12\n",
      "step 12000, training accuracy 1, consistency loss : 0.453011, code_loss 0.756555, reconstr_loss 229.212\n",
      "step 12100, training accuracy 1, consistency loss : 0.44471, code_loss 0.773302, reconstr_loss 227.164\n",
      "step 12200, training accuracy 0.98, consistency loss : 0.458334, code_loss 0.69777, reconstr_loss 235.676\n",
      "step 12300, training accuracy 1, consistency loss : 0.458449, code_loss 0.751139, reconstr_loss 238.348\n",
      "step 12400, training accuracy 1, consistency loss : 0.44609, code_loss 0.768458, reconstr_loss 229.638\n",
      "step 12500, training accuracy 1, consistency loss : 0.460015, code_loss 0.746477, reconstr_loss 236.55\n",
      "step 12600, training accuracy 1, consistency loss : 0.429116, code_loss 0.795487, reconstr_loss 233.857\n",
      "step 12700, training accuracy 1, consistency loss : 0.480162, code_loss 0.731378, reconstr_loss 225.83\n",
      "step 12800, training accuracy 1, consistency loss : 0.446646, code_loss 0.761177, reconstr_loss 223.869\n",
      "step 12900, training accuracy 0.98, consistency loss : 0.442591, code_loss 0.727093, reconstr_loss 229.404\n",
      "step 13000, training accuracy 1, consistency loss : 0.450587, code_loss 0.759707, reconstr_loss 233.287\n",
      "step 13100, training accuracy 1, consistency loss : 0.458953, code_loss 0.750821, reconstr_loss 222.521\n",
      "step 13200, training accuracy 1, consistency loss : 0.472291, code_loss 0.735631, reconstr_loss 220.653\n",
      "step 13300, training accuracy 1, consistency loss : 0.473785, code_loss 0.733046, reconstr_loss 230.589\n",
      "step 13400, training accuracy 1, consistency loss : 0.446776, code_loss 0.759187, reconstr_loss 219.435\n",
      "step 13500, training accuracy 1, consistency loss : 0.460543, code_loss 0.746587, reconstr_loss 221.753\n",
      "step 13600, training accuracy 1, consistency loss : 0.449132, code_loss 0.760957, reconstr_loss 214.339\n",
      "step 13700, training accuracy 1, consistency loss : 0.475799, code_loss 0.732049, reconstr_loss 214.984\n",
      "step 13800, training accuracy 1, consistency loss : 0.438231, code_loss 0.771239, reconstr_loss 214.976\n",
      "step 13900, training accuracy 1, consistency loss : 0.468562, code_loss 0.742538, reconstr_loss 224.836\n",
      "step 14000, training accuracy 1, consistency loss : 0.449035, code_loss 0.758609, reconstr_loss 217.906\n",
      "step 14100, training accuracy 1, consistency loss : 0.474013, code_loss 0.734234, reconstr_loss 209.61\n",
      "step 14200, training accuracy 1, consistency loss : 0.459706, code_loss 0.748839, reconstr_loss 212.782\n",
      "step 14300, training accuracy 1, consistency loss : 0.454927, code_loss 0.751867, reconstr_loss 224.717\n",
      "step 14400, training accuracy 1, consistency loss : 0.469296, code_loss 0.735901, reconstr_loss 216.846\n",
      "step 14500, training accuracy 1, consistency loss : 0.468052, code_loss 0.739173, reconstr_loss 207.369\n",
      "step 14600, training accuracy 1, consistency loss : 0.448719, code_loss 0.758938, reconstr_loss 211.565\n",
      "step 14700, training accuracy 0.98, consistency loss : 0.458486, code_loss 0.694532, reconstr_loss 215.356\n",
      "step 14800, training accuracy 1, consistency loss : 0.476873, code_loss 0.731649, reconstr_loss 222.699\n",
      "step 14900, training accuracy 1, consistency loss : 0.468712, code_loss 0.736766, reconstr_loss 208.702\n",
      "step 15000, training accuracy 1, consistency loss : 0.456786, code_loss 0.749726, reconstr_loss 217.045\n",
      "step 15100, training accuracy 0.98, consistency loss : 0.442254, code_loss 0.713739, reconstr_loss 213.235\n",
      "step 15200, training accuracy 1, consistency loss : 0.447612, code_loss 0.765981, reconstr_loss 218.161\n",
      "step 15300, training accuracy 1, consistency loss : 0.469028, code_loss 0.737705, reconstr_loss 210.331\n",
      "step 15400, training accuracy 1, consistency loss : 0.482603, code_loss 0.722963, reconstr_loss 218.116\n",
      "step 15500, training accuracy 1, consistency loss : 0.486549, code_loss 0.720308, reconstr_loss 215.026\n",
      "step 15600, training accuracy 1, consistency loss : 0.463406, code_loss 0.743307, reconstr_loss 213.161\n",
      "step 15700, training accuracy 1, consistency loss : 0.478686, code_loss 0.727344, reconstr_loss 195.644\n",
      "step 15800, training accuracy 1, consistency loss : 0.477269, code_loss 0.729585, reconstr_loss 215.518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15900, training accuracy 1, consistency loss : 0.44943, code_loss 0.758739, reconstr_loss 200.468\n",
      "step 16000, training accuracy 1, consistency loss : 0.465375, code_loss 0.741696, reconstr_loss 207.848\n",
      "step 16100, training accuracy 1, consistency loss : 0.44967, code_loss 0.759628, reconstr_loss 212.093\n",
      "step 16200, training accuracy 1, consistency loss : 0.45008, code_loss 0.755776, reconstr_loss 199.311\n",
      "step 16300, training accuracy 0.98, consistency loss : 0.433175, code_loss 0.712516, reconstr_loss 196.906\n",
      "step 16400, training accuracy 1, consistency loss : 0.461725, code_loss 0.746197, reconstr_loss 206.833\n",
      "step 16500, training accuracy 1, consistency loss : 0.488065, code_loss 0.719333, reconstr_loss 196.669\n",
      "step 16600, training accuracy 1, consistency loss : 0.474694, code_loss 0.730181, reconstr_loss 207.869\n",
      "step 16700, training accuracy 1, consistency loss : 0.462231, code_loss 0.743948, reconstr_loss 209.492\n",
      "step 16800, training accuracy 1, consistency loss : 0.467875, code_loss 0.737736, reconstr_loss 206.188\n",
      "step 16900, training accuracy 1, consistency loss : 0.464192, code_loss 0.741522, reconstr_loss 192.884\n",
      "step 17000, training accuracy 1, consistency loss : 0.472011, code_loss 0.732765, reconstr_loss 198.908\n",
      "step 17100, training accuracy 1, consistency loss : 0.448976, code_loss 0.756967, reconstr_loss 198.949\n",
      "step 17200, training accuracy 1, consistency loss : 0.443404, code_loss 0.763102, reconstr_loss 198.143\n",
      "step 17300, training accuracy 1, consistency loss : 0.427695, code_loss 0.790824, reconstr_loss 198.456\n",
      "step 17400, training accuracy 1, consistency loss : 0.461077, code_loss 0.748186, reconstr_loss 194.621\n",
      "step 17500, training accuracy 1, consistency loss : 0.45188, code_loss 0.753252, reconstr_loss 197.155\n",
      "step 17600, training accuracy 1, consistency loss : 0.44927, code_loss 0.756109, reconstr_loss 186.948\n",
      "step 17700, training accuracy 1, consistency loss : 0.466731, code_loss 0.738998, reconstr_loss 193.257\n",
      "step 17800, training accuracy 1, consistency loss : 0.446593, code_loss 0.760621, reconstr_loss 196.472\n",
      "step 17900, training accuracy 1, consistency loss : 0.449763, code_loss 0.754982, reconstr_loss 192.405\n",
      "step 18000, training accuracy 1, consistency loss : 0.461235, code_loss 0.74495, reconstr_loss 195.359\n",
      "step 18100, training accuracy 0.98, consistency loss : 0.454819, code_loss 0.693509, reconstr_loss 201.236\n",
      "step 18200, training accuracy 1, consistency loss : 0.445633, code_loss 0.759541, reconstr_loss 198.867\n",
      "step 18300, training accuracy 1, consistency loss : 0.452266, code_loss 0.753835, reconstr_loss 202.743\n",
      "step 18400, training accuracy 1, consistency loss : 0.434782, code_loss 0.771206, reconstr_loss 187.841\n",
      "step 18500, training accuracy 1, consistency loss : 0.46756, code_loss 0.737346, reconstr_loss 203.18\n",
      "step 18600, training accuracy 1, consistency loss : 0.465078, code_loss 0.7399, reconstr_loss 195.44\n",
      "step 18700, training accuracy 1, consistency loss : 0.46079, code_loss 0.743987, reconstr_loss 184.121\n",
      "step 18800, training accuracy 1, consistency loss : 0.453473, code_loss 0.752609, reconstr_loss 191.112\n",
      "step 18900, training accuracy 1, consistency loss : 0.449737, code_loss 0.754829, reconstr_loss 188.957\n",
      "step 19000, training accuracy 1, consistency loss : 0.460552, code_loss 0.744287, reconstr_loss 191.783\n",
      "step 19100, training accuracy 1, consistency loss : 0.450723, code_loss 0.753828, reconstr_loss 186.601\n",
      "step 19200, training accuracy 1, consistency loss : 0.466339, code_loss 0.73755, reconstr_loss 197.155\n",
      "step 19300, training accuracy 1, consistency loss : 0.453178, code_loss 0.752696, reconstr_loss 182.404\n",
      "step 19400, training accuracy 1, consistency loss : 0.453512, code_loss 0.750921, reconstr_loss 194.068\n",
      "step 19500, training accuracy 1, consistency loss : 0.469417, code_loss 0.735246, reconstr_loss 202.531\n",
      "step 19600, training accuracy 1, consistency loss : 0.463404, code_loss 0.741992, reconstr_loss 188.781\n",
      "step 19700, training accuracy 1, consistency loss : 0.476909, code_loss 0.728451, reconstr_loss 195.826\n",
      "step 19800, training accuracy 1, consistency loss : 0.474497, code_loss 0.731267, reconstr_loss 194.197\n",
      "step 19900, training accuracy 1, consistency loss : 0.448841, code_loss 0.755468, reconstr_loss 197.66\n"
     ]
    }
   ],
   "source": [
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    #print(reward.eval(feed_dict={x: batch[0], y_: batch[1]}))\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1]}) #, keep_prob: 1.0})\n",
    "        consistency_loss = np.mean(KL.eval(feed_dict={x: batch[0], y_: batch[1]})) #, keep_prob: 1.0})\n",
    "        code_loss = np.mean(cross_entropy_loss.eval(feed_dict={x: batch[0], y_: batch[1]})) #, keep_prob: 1.0})\n",
    "        reco_loss = np.mean(reconstruction_loss.eval(feed_dict={x: batch[0], y_: batch[1]})) #, keep_prob: 1.0})\n",
    "        print('step %d, training accuracy %g, consistency loss : %g, code_loss %g, reconstr_loss %g' % (i, train_accuracy, consistency_loss, code_loss,  reco_loss))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]}) #, keep_prob: 0.5})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 8, 0, 6, 6, 1, 1, 8, 8, 6, 8, 9, 7, 2, 1, 1, 7, 3, 2, 7, 1, 2, 4,\n",
       "       2, 0, 9, 6, 6, 1, 8, 1, 7, 6, 9, 1, 7, 9, 6, 2, 1, 5, 0, 8, 1, 0, 0,\n",
       "       7, 2, 1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cross_entropy_loss.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "indices.eval(feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "code = sess.run(tf.nn.softmax(y_logit), feed_dict={x: batch[0], y_: batch[1]})\n",
    "pred = sess.run(x_gen, feed_dict={x: batch[0], y_: batch[1]})\n",
    "i = 0\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(np.reshape(batch[0][i],(28, 28)))\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(code[i])\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(np.reshape(pred[i],(28, 28)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run((tf.multinomial(y_logit, 1)[:,0], indices), feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.log(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
