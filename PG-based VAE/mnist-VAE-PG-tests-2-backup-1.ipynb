{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## La couche d'encodage est le label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_LABEL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    stddev = np.sqrt(1. / np.prod(shape[:-1]))\n",
    "    print(stddev)\n",
    "    initial = tf.random_normal(shape, stddev = stddev)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0., shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, NB_LABEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0353553390593\n"
     ]
    }
   ],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0178571428571\n"
     ]
    }
   ],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03125\n"
     ]
    }
   ],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_logit = tf.matmul(h_fc1, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tf.argmax(y_logit, 1)\n",
    "\n",
    "# multinomial softmax draw\n",
    "#indices = tf.multinomial(y_logit, 1)[:,0]\n",
    "\n",
    "depth = NB_LABEL\n",
    "y_hat = tf.one_hot(indices, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.316227766017\n"
     ]
    }
   ],
   "source": [
    "'''W_fc3 = weight_variable([10, 1024])\n",
    "b_fc3 = bias_variable([1024])\n",
    "\n",
    "#h_decoder = tf.nn.relu(tf.matmul(tf.nn.softmax(y_logit), W_fc3) + b_fc3)\n",
    "h_decoder = tf.nn.relu(tf.matmul(y_hat * y_logit, W_fc3) + b_fc3)\n",
    "#h_decoder = tf.matmul(y_hat, W_fc3) + b_fc3\n",
    "\n",
    "W_fc4 = weight_variable([1024, 784])\n",
    "b_fc4 = bias_variable([784])\n",
    "\n",
    "x_gen = tf.matmul(h_decoder, W_fc4) + b_fc4'''\n",
    "\n",
    "\n",
    "W_fc3 = weight_variable([10, 784])\n",
    "b_fc3 = bias_variable([784])\n",
    "#x_gen = tf.matmul(tf.nn.softmax(y_logit), W_fc3) + b_fc3\n",
    "x_gen = tf.matmul(y_hat, W_fc3) + b_fc3\n",
    "\n",
    "#x_gen = tf.matmul(y_hat * y_logit, W_fc3) + b_fc3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.cast(tf.equal(indices, tf.argmax(y_, 1)), tf.float32)\n",
    "reward = correct_prediction - (1 - correct_prediction)\n",
    "#reward_KL_plus = 1 #correct_prediction\n",
    "#reward_KL_moins = 1 - correct_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "p = tf.nn.softmax(y_logit)\n",
    "q = 1. / NB_LABEL\n",
    "\n",
    "#KL = tf.reduce_sum(p * y_logit,reduction_indices=1)\n",
    "KL_ref = tf.reduce_sum(p * tf.log(p/q), reduction_indices=1)\n",
    "KL = tf.reduce_mean(KL_ref) # + reward_KL_moins * (- tf.log(0.1) - KL_ref))\n",
    "#KL = tf.nn.softmax_cross_entropy_with_logits(labels=y_hat, logits=y_logit) #!! KL(1_\\hat{y})\n",
    "#KL = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_logit)) #!! KL(1_\\hat{y})\n",
    "\n",
    "#cross_entropy_loss = tf.reduce_mean(reward * tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_gen, labels=x), reduction_indices=1))\n",
    "#reward = 1\n",
    "cross_entropy_loss = tf.reduce_mean(reward  * tf.nn.softmax_cross_entropy_with_logits(labels=y_hat, logits=y_logit)) \n",
    "#cross_entropy_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_gen, labels=x))\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_gen, labels=x), reduction_indices = 1))\n",
    "#rewarded_loss = reward * (KL + cross_entropy_loss)\n",
    "\n",
    "rewarded_loss = KL +  cross_entropy_loss + reconstruction_loss\n",
    "                              \n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(rewarded_loss)\n",
    "accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.run(KL, feed_dict={x: batch[0], y_: batch[1]})\n",
    "#sess.run(reward, feed_dict={x: batch[0], y_: batch[1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.run(rewarded_cross_entropy, feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.08, consistency loss : 0.0199928, code_loss -1.68037, reconstr_loss 554.237\n",
      "step 100, training accuracy 0.8, consistency loss : 0.306415, code_loss 0.476682, reconstr_loss 547.511\n",
      "step 200, training accuracy 0.9, consistency loss : 0.399056, code_loss 0.663301, reconstr_loss 543.162\n",
      "step 300, training accuracy 0.98, consistency loss : 0.45922, code_loss 0.788514, reconstr_loss 538.489\n",
      "step 400, training accuracy 1, consistency loss : 0.418548, code_loss 0.874703, reconstr_loss 532.4\n",
      "step 500, training accuracy 0.94, consistency loss : 0.403554, code_loss 0.720719, reconstr_loss 527.521\n",
      "step 600, training accuracy 0.98, consistency loss : 0.431441, code_loss 0.798912, reconstr_loss 523.35\n",
      "step 700, training accuracy 1, consistency loss : 0.42575, code_loss 0.850445, reconstr_loss 517.767\n",
      "step 800, training accuracy 0.98, consistency loss : 0.421299, code_loss 0.822363, reconstr_loss 512.466\n",
      "step 900, training accuracy 0.96, consistency loss : 0.429141, code_loss 0.752657, reconstr_loss 507.343\n",
      "step 1000, training accuracy 0.98, consistency loss : 0.450645, code_loss 0.755516, reconstr_loss 502.714\n",
      "step 1100, training accuracy 0.92, consistency loss : 0.38071, code_loss 0.668553, reconstr_loss 497.684\n",
      "step 1200, training accuracy 1, consistency loss : 0.515457, code_loss 0.717081, reconstr_loss 491.471\n",
      "step 1300, training accuracy 1, consistency loss : 0.43864, code_loss 0.820336, reconstr_loss 486.412\n",
      "step 1400, training accuracy 0.98, consistency loss : 0.48876, code_loss 0.678779, reconstr_loss 483.564\n",
      "step 1500, training accuracy 0.96, consistency loss : 0.394759, code_loss 0.74059, reconstr_loss 477.972\n",
      "step 1600, training accuracy 1, consistency loss : 0.440905, code_loss 0.810264, reconstr_loss 473.179\n",
      "step 1700, training accuracy 0.98, consistency loss : 0.432893, code_loss 0.774088, reconstr_loss 469.94\n",
      "step 1800, training accuracy 0.98, consistency loss : 0.43167, code_loss 0.756929, reconstr_loss 464.555\n",
      "step 1900, training accuracy 0.96, consistency loss : 0.434191, code_loss 0.699966, reconstr_loss 460.495\n",
      "step 2000, training accuracy 1, consistency loss : 0.458811, code_loss 0.782104, reconstr_loss 457.932\n",
      "step 2100, training accuracy 0.98, consistency loss : 0.412912, code_loss 0.769596, reconstr_loss 453.135\n",
      "step 2200, training accuracy 1, consistency loss : 0.49065, code_loss 0.747809, reconstr_loss 449.225\n",
      "step 2300, training accuracy 0.98, consistency loss : 0.428598, code_loss 0.76154, reconstr_loss 441.851\n",
      "step 2400, training accuracy 0.98, consistency loss : 0.410753, code_loss 0.778635, reconstr_loss 438.827\n",
      "step 2500, training accuracy 1, consistency loss : 0.494351, code_loss 0.7258, reconstr_loss 435.669\n",
      "step 2600, training accuracy 1, consistency loss : 0.458097, code_loss 0.766803, reconstr_loss 434.346\n",
      "step 2700, training accuracy 1, consistency loss : 0.435246, code_loss 0.802322, reconstr_loss 431.514\n",
      "step 2800, training accuracy 0.98, consistency loss : 0.445061, code_loss 0.736457, reconstr_loss 422.001\n",
      "step 2900, training accuracy 1, consistency loss : 0.45059, code_loss 0.780614, reconstr_loss 420.747\n",
      "step 3000, training accuracy 1, consistency loss : 0.448773, code_loss 0.781821, reconstr_loss 417.869\n",
      "step 3100, training accuracy 0.96, consistency loss : 0.419367, code_loss 0.706304, reconstr_loss 415.341\n",
      "step 3200, training accuracy 1, consistency loss : 0.469908, code_loss 0.755224, reconstr_loss 408.086\n",
      "step 3300, training accuracy 1, consistency loss : 0.47696, code_loss 0.737921, reconstr_loss 409.567\n",
      "step 3400, training accuracy 0.98, consistency loss : 0.481954, code_loss 0.692483, reconstr_loss 402.403\n",
      "step 3500, training accuracy 1, consistency loss : 0.463274, code_loss 0.751063, reconstr_loss 402.514\n",
      "step 3600, training accuracy 1, consistency loss : 0.440931, code_loss 0.783519, reconstr_loss 396.585\n",
      "step 3700, training accuracy 0.98, consistency loss : 0.435198, code_loss 0.737728, reconstr_loss 395.12\n",
      "step 3800, training accuracy 1, consistency loss : 0.501783, code_loss 0.717468, reconstr_loss 386.814\n",
      "step 3900, training accuracy 1, consistency loss : 0.504862, code_loss 0.720265, reconstr_loss 386.035\n",
      "step 4000, training accuracy 0.98, consistency loss : 0.421777, code_loss 0.747775, reconstr_loss 383.463\n",
      "step 4100, training accuracy 1, consistency loss : 0.457837, code_loss 0.768589, reconstr_loss 380.788\n",
      "step 4200, training accuracy 1, consistency loss : 0.459656, code_loss 0.7657, reconstr_loss 379.571\n",
      "step 4300, training accuracy 0.96, consistency loss : 0.458885, code_loss 0.670287, reconstr_loss 372.07\n",
      "step 4400, training accuracy 1, consistency loss : 0.465267, code_loss 0.770597, reconstr_loss 374.469\n",
      "step 4500, training accuracy 0.98, consistency loss : 0.469918, code_loss 0.686127, reconstr_loss 366.102\n",
      "step 4600, training accuracy 1, consistency loss : 0.472152, code_loss 0.759564, reconstr_loss 371.056\n",
      "step 4700, training accuracy 1, consistency loss : 0.455094, code_loss 0.766075, reconstr_loss 360.832\n",
      "step 4800, training accuracy 1, consistency loss : 0.441705, code_loss 0.784676, reconstr_loss 360.642\n",
      "step 4900, training accuracy 1, consistency loss : 0.479192, code_loss 0.741487, reconstr_loss 357.753\n",
      "step 5000, training accuracy 1, consistency loss : 0.44896, code_loss 0.769009, reconstr_loss 355.652\n",
      "step 5100, training accuracy 1, consistency loss : 0.456354, code_loss 0.783882, reconstr_loss 354.619\n",
      "step 5200, training accuracy 1, consistency loss : 0.452837, code_loss 0.758859, reconstr_loss 349.725\n",
      "step 5300, training accuracy 1, consistency loss : 0.46679, code_loss 0.744241, reconstr_loss 344.539\n",
      "step 5400, training accuracy 1, consistency loss : 0.436591, code_loss 0.783906, reconstr_loss 342.911\n",
      "step 5500, training accuracy 1, consistency loss : 0.443327, code_loss 0.780377, reconstr_loss 337.472\n",
      "step 5600, training accuracy 0.98, consistency loss : 0.483879, code_loss 0.694117, reconstr_loss 335.626\n",
      "step 5700, training accuracy 1, consistency loss : 0.456178, code_loss 0.761273, reconstr_loss 338.383\n",
      "step 5800, training accuracy 1, consistency loss : 0.424105, code_loss 0.791673, reconstr_loss 329.493\n",
      "step 5900, training accuracy 1, consistency loss : 0.428634, code_loss 0.787067, reconstr_loss 334.461\n",
      "step 6000, training accuracy 1, consistency loss : 0.488056, code_loss 0.722265, reconstr_loss 325.093\n",
      "step 6100, training accuracy 1, consistency loss : 0.44909, code_loss 0.767084, reconstr_loss 325.93\n",
      "step 6200, training accuracy 0.98, consistency loss : 0.453916, code_loss 0.714143, reconstr_loss 322.008\n",
      "step 6300, training accuracy 1, consistency loss : 0.44197, code_loss 0.78839, reconstr_loss 323.083\n",
      "step 6400, training accuracy 1, consistency loss : 0.461705, code_loss 0.75631, reconstr_loss 322.956\n",
      "step 6500, training accuracy 1, consistency loss : 0.448838, code_loss 0.764256, reconstr_loss 322.419\n",
      "step 6600, training accuracy 1, consistency loss : 0.460779, code_loss 0.751451, reconstr_loss 313.18\n",
      "step 6700, training accuracy 0.98, consistency loss : 0.453332, code_loss 0.714912, reconstr_loss 315.8\n",
      "step 6800, training accuracy 1, consistency loss : 0.467881, code_loss 0.747903, reconstr_loss 312.197\n",
      "step 6900, training accuracy 1, consistency loss : 0.440899, code_loss 0.780004, reconstr_loss 304.963\n",
      "step 7000, training accuracy 0.98, consistency loss : 0.469566, code_loss 0.693514, reconstr_loss 305.863\n",
      "step 7100, training accuracy 1, consistency loss : 0.447492, code_loss 0.762532, reconstr_loss 303.655\n",
      "step 7200, training accuracy 1, consistency loss : 0.487139, code_loss 0.722894, reconstr_loss 302.696\n",
      "step 7300, training accuracy 1, consistency loss : 0.450021, code_loss 0.766466, reconstr_loss 300.171\n",
      "step 7400, training accuracy 1, consistency loss : 0.45218, code_loss 0.762054, reconstr_loss 301.16\n",
      "step 7500, training accuracy 1, consistency loss : 0.456958, code_loss 0.752168, reconstr_loss 293.334\n",
      "step 7600, training accuracy 1, consistency loss : 0.474881, code_loss 0.737013, reconstr_loss 296.373\n",
      "step 7700, training accuracy 1, consistency loss : 0.457152, code_loss 0.756065, reconstr_loss 295.911\n",
      "step 7800, training accuracy 1, consistency loss : 0.445345, code_loss 0.767076, reconstr_loss 291.022\n",
      "step 7900, training accuracy 1, consistency loss : 0.445333, code_loss 0.776533, reconstr_loss 287.115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8000, training accuracy 1, consistency loss : 0.459303, code_loss 0.752703, reconstr_loss 282.257\n",
      "step 8100, training accuracy 1, consistency loss : 0.442037, code_loss 0.781355, reconstr_loss 286.358\n",
      "step 8200, training accuracy 1, consistency loss : 0.468671, code_loss 0.743004, reconstr_loss 280.726\n",
      "step 8300, training accuracy 1, consistency loss : 0.455671, code_loss 0.754676, reconstr_loss 285.906\n",
      "step 8400, training accuracy 1, consistency loss : 0.437628, code_loss 0.784845, reconstr_loss 271.192\n",
      "step 8500, training accuracy 1, consistency loss : 0.466589, code_loss 0.743969, reconstr_loss 275.507\n",
      "step 8600, training accuracy 1, consistency loss : 0.450002, code_loss 0.770517, reconstr_loss 281.849\n",
      "step 8700, training accuracy 0.98, consistency loss : 0.453451, code_loss 0.706267, reconstr_loss 285.341\n",
      "step 8800, training accuracy 1, consistency loss : 0.465583, code_loss 0.747024, reconstr_loss 268.8\n",
      "step 8900, training accuracy 1, consistency loss : 0.478524, code_loss 0.731504, reconstr_loss 275.705\n",
      "step 9000, training accuracy 1, consistency loss : 0.427437, code_loss 0.785064, reconstr_loss 266.421\n",
      "step 9100, training accuracy 1, consistency loss : 0.447757, code_loss 0.759995, reconstr_loss 278.683\n",
      "step 9200, training accuracy 1, consistency loss : 0.458499, code_loss 0.753794, reconstr_loss 275.732\n",
      "step 9300, training accuracy 1, consistency loss : 0.465005, code_loss 0.7464, reconstr_loss 266.64\n",
      "step 9400, training accuracy 1, consistency loss : 0.464729, code_loss 0.744406, reconstr_loss 262.521\n",
      "step 9500, training accuracy 1, consistency loss : 0.452227, code_loss 0.756017, reconstr_loss 256.952\n",
      "step 9600, training accuracy 1, consistency loss : 0.451583, code_loss 0.760693, reconstr_loss 260.33\n",
      "step 9700, training accuracy 1, consistency loss : 0.434202, code_loss 0.779808, reconstr_loss 257.815\n",
      "step 9800, training accuracy 1, consistency loss : 0.473338, code_loss 0.743193, reconstr_loss 263.323\n",
      "step 9900, training accuracy 1, consistency loss : 0.440536, code_loss 0.773983, reconstr_loss 263.661\n",
      "step 10000, training accuracy 1, consistency loss : 0.450681, code_loss 0.765038, reconstr_loss 251.917\n",
      "step 10100, training accuracy 1, consistency loss : 0.47235, code_loss 0.735949, reconstr_loss 252.375\n",
      "step 10200, training accuracy 1, consistency loss : 0.448237, code_loss 0.76239, reconstr_loss 250.809\n",
      "step 10300, training accuracy 1, consistency loss : 0.452002, code_loss 0.758294, reconstr_loss 251.392\n",
      "step 10400, training accuracy 1, consistency loss : 0.446724, code_loss 0.76705, reconstr_loss 245.225\n",
      "step 10500, training accuracy 1, consistency loss : 0.462475, code_loss 0.752148, reconstr_loss 263.347\n",
      "step 10600, training accuracy 1, consistency loss : 0.45944, code_loss 0.752457, reconstr_loss 239.032\n",
      "step 10700, training accuracy 1, consistency loss : 0.447109, code_loss 0.763745, reconstr_loss 248.136\n",
      "step 10800, training accuracy 1, consistency loss : 0.457024, code_loss 0.752538, reconstr_loss 245.607\n",
      "step 10900, training accuracy 1, consistency loss : 0.45239, code_loss 0.755749, reconstr_loss 243.943\n",
      "step 11000, training accuracy 1, consistency loss : 0.451841, code_loss 0.761411, reconstr_loss 240.462\n",
      "step 11100, training accuracy 1, consistency loss : 0.463192, code_loss 0.746314, reconstr_loss 243.785\n",
      "step 11200, training accuracy 1, consistency loss : 0.449297, code_loss 0.759178, reconstr_loss 238.706\n",
      "step 11300, training accuracy 1, consistency loss : 0.447311, code_loss 0.760691, reconstr_loss 236.353\n",
      "step 11400, training accuracy 1, consistency loss : 0.437069, code_loss 0.772375, reconstr_loss 242.126\n",
      "step 11500, training accuracy 1, consistency loss : 0.438203, code_loss 0.771983, reconstr_loss 240.456\n",
      "step 11600, training accuracy 1, consistency loss : 0.446592, code_loss 0.760808, reconstr_loss 240.601\n",
      "step 11700, training accuracy 1, consistency loss : 0.456548, code_loss 0.751841, reconstr_loss 230.147\n",
      "step 11800, training accuracy 1, consistency loss : 0.47676, code_loss 0.733457, reconstr_loss 231.723\n",
      "step 11900, training accuracy 1, consistency loss : 0.445844, code_loss 0.765158, reconstr_loss 228.925\n",
      "step 12000, training accuracy 1, consistency loss : 0.457592, code_loss 0.75018, reconstr_loss 218.528\n",
      "step 12100, training accuracy 1, consistency loss : 0.454702, code_loss 0.754312, reconstr_loss 228.914\n",
      "step 12200, training accuracy 1, consistency loss : 0.463414, code_loss 0.746053, reconstr_loss 234.562\n",
      "step 12300, training accuracy 1, consistency loss : 0.462284, code_loss 0.747007, reconstr_loss 220.974\n",
      "step 12400, training accuracy 1, consistency loss : 0.464476, code_loss 0.742775, reconstr_loss 226.254\n",
      "step 12500, training accuracy 1, consistency loss : 0.453198, code_loss 0.752059, reconstr_loss 215.265\n",
      "step 12600, training accuracy 1, consistency loss : 0.467877, code_loss 0.738071, reconstr_loss 232.98\n",
      "step 12700, training accuracy 1, consistency loss : 0.458521, code_loss 0.748439, reconstr_loss 221.858\n",
      "step 12800, training accuracy 1, consistency loss : 0.466384, code_loss 0.745009, reconstr_loss 225.782\n",
      "step 12900, training accuracy 1, consistency loss : 0.468128, code_loss 0.741386, reconstr_loss 218.094\n",
      "step 13000, training accuracy 1, consistency loss : 0.454598, code_loss 0.757677, reconstr_loss 223.234\n",
      "step 13100, training accuracy 1, consistency loss : 0.45059, code_loss 0.758145, reconstr_loss 223.101\n",
      "step 13200, training accuracy 1, consistency loss : 0.481018, code_loss 0.726143, reconstr_loss 228.089\n",
      "step 13300, training accuracy 1, consistency loss : 0.454047, code_loss 0.753743, reconstr_loss 221.448\n",
      "step 13400, training accuracy 1, consistency loss : 0.458555, code_loss 0.74938, reconstr_loss 213.925\n",
      "step 13500, training accuracy 1, consistency loss : 0.459119, code_loss 0.747083, reconstr_loss 220.142\n",
      "step 13600, training accuracy 1, consistency loss : 0.470983, code_loss 0.73634, reconstr_loss 226.77\n",
      "step 13700, training accuracy 1, consistency loss : 0.451576, code_loss 0.754618, reconstr_loss 216.095\n",
      "step 13800, training accuracy 1, consistency loss : 0.46617, code_loss 0.740927, reconstr_loss 227.771\n",
      "step 13900, training accuracy 1, consistency loss : 0.465038, code_loss 0.741866, reconstr_loss 220.003\n",
      "step 14000, training accuracy 1, consistency loss : 0.44825, code_loss 0.759578, reconstr_loss 221.719\n",
      "step 14100, training accuracy 1, consistency loss : 0.44794, code_loss 0.760191, reconstr_loss 215.344\n",
      "step 14200, training accuracy 1, consistency loss : 0.445473, code_loss 0.760323, reconstr_loss 210.292\n",
      "step 14300, training accuracy 1, consistency loss : 0.454321, code_loss 0.751889, reconstr_loss 211.157\n",
      "step 14400, training accuracy 1, consistency loss : 0.462654, code_loss 0.742687, reconstr_loss 217.398\n",
      "step 14500, training accuracy 1, consistency loss : 0.455306, code_loss 0.750329, reconstr_loss 213.689\n",
      "step 14600, training accuracy 1, consistency loss : 0.446969, code_loss 0.758556, reconstr_loss 208.077\n",
      "step 14700, training accuracy 1, consistency loss : 0.45377, code_loss 0.752961, reconstr_loss 212.638\n",
      "step 14800, training accuracy 1, consistency loss : 0.442759, code_loss 0.7649, reconstr_loss 212.519\n",
      "step 14900, training accuracy 1, consistency loss : 0.458071, code_loss 0.747341, reconstr_loss 207.084\n",
      "step 15000, training accuracy 1, consistency loss : 0.449339, code_loss 0.756857, reconstr_loss 212.226\n",
      "step 15100, training accuracy 1, consistency loss : 0.442858, code_loss 0.766648, reconstr_loss 206.261\n",
      "step 15200, training accuracy 1, consistency loss : 0.454928, code_loss 0.766574, reconstr_loss 208.984\n",
      "step 15300, training accuracy 1, consistency loss : 0.430204, code_loss 0.777349, reconstr_loss 217.001\n",
      "step 15400, training accuracy 1, consistency loss : 0.460028, code_loss 0.746752, reconstr_loss 213.413\n",
      "step 15500, training accuracy 1, consistency loss : 0.459654, code_loss 0.748343, reconstr_loss 207.483\n",
      "step 15600, training accuracy 1, consistency loss : 0.457679, code_loss 0.74893, reconstr_loss 209.191\n",
      "step 15700, training accuracy 1, consistency loss : 0.459653, code_loss 0.745132, reconstr_loss 206.841\n",
      "step 15800, training accuracy 0.98, consistency loss : 0.462684, code_loss 0.679653, reconstr_loss 209.196\n",
      "step 15900, training accuracy 1, consistency loss : 0.46006, code_loss 0.745356, reconstr_loss 210.438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16000, training accuracy 1, consistency loss : 0.44479, code_loss 0.760324, reconstr_loss 198.167\n",
      "step 16100, training accuracy 1, consistency loss : 0.455786, code_loss 0.750142, reconstr_loss 210.007\n",
      "step 16200, training accuracy 1, consistency loss : 0.455217, code_loss 0.750424, reconstr_loss 204.489\n",
      "step 16300, training accuracy 1, consistency loss : 0.448898, code_loss 0.757095, reconstr_loss 208.679\n",
      "step 16400, training accuracy 1, consistency loss : 0.4602, code_loss 0.745673, reconstr_loss 193.329\n",
      "step 16500, training accuracy 1, consistency loss : 0.45759, code_loss 0.748731, reconstr_loss 204.745\n",
      "step 16600, training accuracy 1, consistency loss : 0.459419, code_loss 0.748539, reconstr_loss 202.996\n",
      "step 16700, training accuracy 1, consistency loss : 0.476185, code_loss 0.727906, reconstr_loss 206.513\n",
      "step 16800, training accuracy 1, consistency loss : 0.463005, code_loss 0.742511, reconstr_loss 212.809\n",
      "step 16900, training accuracy 1, consistency loss : 0.453242, code_loss 0.7526, reconstr_loss 194.389\n",
      "step 17000, training accuracy 1, consistency loss : 0.455946, code_loss 0.749031, reconstr_loss 201.065\n",
      "step 17100, training accuracy 1, consistency loss : 0.479993, code_loss 0.725286, reconstr_loss 206.202\n",
      "step 17200, training accuracy 1, consistency loss : 0.460442, code_loss 0.745189, reconstr_loss 205.916\n",
      "step 17300, training accuracy 1, consistency loss : 0.453017, code_loss 0.752723, reconstr_loss 213.467\n",
      "step 17400, training accuracy 1, consistency loss : 0.445657, code_loss 0.759119, reconstr_loss 199.615\n",
      "step 17500, training accuracy 1, consistency loss : 0.456433, code_loss 0.755411, reconstr_loss 195.261\n",
      "step 17600, training accuracy 1, consistency loss : 0.436703, code_loss 0.772133, reconstr_loss 186.398\n",
      "step 17700, training accuracy 1, consistency loss : 0.450696, code_loss 0.756841, reconstr_loss 196.462\n",
      "step 17800, training accuracy 1, consistency loss : 0.459517, code_loss 0.746025, reconstr_loss 187.593\n",
      "step 17900, training accuracy 1, consistency loss : 0.444214, code_loss 0.76188, reconstr_loss 202.91\n",
      "step 18000, training accuracy 1, consistency loss : 0.445082, code_loss 0.759717, reconstr_loss 202.28\n",
      "step 18100, training accuracy 1, consistency loss : 0.476266, code_loss 0.728877, reconstr_loss 199.278\n",
      "step 18200, training accuracy 1, consistency loss : 0.453953, code_loss 0.751068, reconstr_loss 196.634\n",
      "step 18300, training accuracy 1, consistency loss : 0.467425, code_loss 0.737655, reconstr_loss 197.132\n",
      "step 18400, training accuracy 1, consistency loss : 0.433756, code_loss 0.773152, reconstr_loss 189.425\n",
      "step 18500, training accuracy 1, consistency loss : 0.453094, code_loss 0.753614, reconstr_loss 198.048\n",
      "step 18600, training accuracy 1, consistency loss : 0.467764, code_loss 0.736671, reconstr_loss 200.938\n",
      "step 18700, training accuracy 1, consistency loss : 0.453441, code_loss 0.752891, reconstr_loss 203.222\n",
      "step 18800, training accuracy 1, consistency loss : 0.467946, code_loss 0.739316, reconstr_loss 208.753\n",
      "step 18900, training accuracy 1, consistency loss : 0.452174, code_loss 0.752309, reconstr_loss 192.563\n",
      "step 19000, training accuracy 1, consistency loss : 0.449164, code_loss 0.755979, reconstr_loss 191.259\n",
      "step 19100, training accuracy 1, consistency loss : 0.471171, code_loss 0.733234, reconstr_loss 193.058\n",
      "step 19200, training accuracy 1, consistency loss : 0.466378, code_loss 0.73908, reconstr_loss 199.395\n",
      "step 19300, training accuracy 1, consistency loss : 0.45483, code_loss 0.749348, reconstr_loss 191.819\n",
      "step 19400, training accuracy 1, consistency loss : 0.483931, code_loss 0.723153, reconstr_loss 195.128\n",
      "step 19500, training accuracy 1, consistency loss : 0.463951, code_loss 0.742827, reconstr_loss 194.43\n",
      "step 19600, training accuracy 1, consistency loss : 0.476211, code_loss 0.729422, reconstr_loss 181.713\n",
      "step 19700, training accuracy 1, consistency loss : 0.451868, code_loss 0.753142, reconstr_loss 184.127\n",
      "step 19800, training accuracy 1, consistency loss : 0.457247, code_loss 0.747679, reconstr_loss 191.632\n",
      "step 19900, training accuracy 1, consistency loss : 0.463892, code_loss 0.740968, reconstr_loss 199.371\n"
     ]
    }
   ],
   "source": [
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    #print(reward.eval(feed_dict={x: batch[0], y_: batch[1]}))\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1]}) #, keep_prob: 1.0})\n",
    "        consistency_loss = np.mean(KL.eval(feed_dict={x: batch[0], y_: batch[1]})) #, keep_prob: 1.0})\n",
    "        code_loss = np.mean(cross_entropy_loss.eval(feed_dict={x: batch[0], y_: batch[1]})) #, keep_prob: 1.0})\n",
    "        reco_loss = np.mean(reconstruction_loss.eval(feed_dict={x: batch[0], y_: batch[1]})) #, keep_prob: 1.0})\n",
    "        print('step %d, training accuracy %g, consistency loss : %g, code_loss %g, reconstr_loss %g' % (i, train_accuracy, consistency_loss, code_loss,  reco_loss))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]}) #, keep_prob: 0.5})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-553cbe714639>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#cross_entropy_loss.eval(feed_dict={x: batch[0], y_: batch[1]})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'indices' is not defined"
     ]
    }
   ],
   "source": [
    "#cross_entropy_loss.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "indices.eval(feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.994\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3edea26b50>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAAJCCAYAAABXmtfhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XtwrOldH/jf061uqXXOkXruM54Ze8b4EjsJ5nIwLNRu\nJVyyJhcTKpeFXAqyWZxk43WoJLvFZjdki2xlKZLAkuCicBIvJCGQe+JkHQi4yBJiDB5jY2xsM7ax\nmRnPfUYtnaOW1Jdn/5D6zPHMOT7voyOp37f786mamiPpp6d/arVa/X71XFLOOQAAAABYbK15NwAA\nAADA6RMCAQAAACwBIRAAAADAEhACAQAAACwBIRAAAADAEhACAQAAACwBIRAAAADAEhACAQAAACwB\nIRAAAADAEliZdwMANFNK6U0R8YMR0Y6Iv59z/t4XffzbI+JvRsRjR+/6oZzz3/9CY95+++35gQce\nOPlmARruAx/4wDM55zvm3QcAzSYEAqBYSqkdEW+PiG+IiEcj4v0ppXflnH/9RaX/NOf81qrjPvDA\nA/HQQw+dYKcAiyGl9Nl59wBA81kOBsBxvDEiPplz/nTO+SAifjIivmnOPQEAAF+AEAiA47g3Ih65\n6u1Hj973Yn8opfThlNK/SCndf62BUkpvSSk9lFJ66Omnnz6NXgEAgBACAXA86Rrvyy96+99FxAM5\n5y+OiJ+NiB+71kA553fknC/mnC/ecYftLgAA4LQIgQA4jkcj4uqZPfdFxOeuLsg5P5tz3j968+9F\nxJefUW8AAMA1CIEAOI73R8SrU0oPppS6EfEtEfGuqwtSSvdc9eabI+JjZ9gfAADwIk4HA6BYznmc\nUnprRPx0HB4R/86c80dTSt8TEQ/lnN8VEW9LKb05IsYR8VxEfPvcGgYAACLl/OItHABgPi5evJgd\nEQ/wUimlD+ScL867DwCazXIwAAAAgCUgBAIAAABYAkIgAAAAgCUgBAIAAABYAkIgAAAAgCUgBAIA\nAABYAkIgAAAAgCUgBAIAAABYAkIgAAAAgCUgBAIAAABYAkIgAAAAgCUgBAIAAABYAkIgAAAAgCUg\nBAIAAABYAkIgAAAAgCUgBAIAWEDjyTT+3D/+QPzqI1vzbgUAqAkhEADAAnpqZz/+w0eeiF/45DPz\nbgUAqAkhEADAAhoMRxERsX30fwAAIRAAwAKahUBbu0IgAOCQEAgAYAHNQqCBmUAAwBEhEADAAhrs\nCoEAgM8nBAIAWEBXloMJgQCAI0IgAIAFZGNoAODFhEAAAAvInkAAwIsJgQAAFtAs/Lm0P47RZDrn\nbgCAOhACAQAsoKtnAFkSBgBECIEAABbS1RtCWxIGAEQIgQAAFtL2cBQXVlciwglhAMAhIRAAwAIa\nDEfx8tvWr/wbAEAIBACwYHLOhyHQrYchkD2BAIAIIRAAwMK5fDCJyTRfCYG2doVAAIAQCABg4Wzt\nHkRExP23Wg4GALxACAQAsGBmoc9t57pxrtsWAgEAESEEAgBYOLPQZ3O9E/31ruVgAEBECIEAABbO\nbCPozV4nNnodM4EAgIgQAgEALJzBVSHQZm/F6WAAQEQIgQAAFs7VIVC/1zUTCACICCEQAMDC2dod\nRbuV4vzqSmz2OrE1PJh3SwBADQiBAAAWzGA4is1eJ1JKsbluTyAA4JAQCABgwcxCoIjDJWF7o2ns\njSZz7goAmDchEADAghkMR7FxVQgUETaHBgCEQAAAi2b7RTOBIsKSMABACAQAsGiuXg7WXxcCAQCH\nhEAAAAtmaziKzd5KRLwwE2hrVwgEAMtOCAQAsECm0xzbw1H0e92IsBwMAHiBEAgAYIFcOhjHNL8Q\n/szCICEQACAEAgBYIIOjZV+zEOjC2kqkdLhEDABYbkIgAIAFMpvxMzsivtVKcWF1xRHxAIAQCABg\nkcxCoNlMoIiI/nrXcjAAQAgEALBIrhUCbfY6sbV7MK+WAICaEAIBACyQWQjUX//8EMhMIABACAQA\nsECuORNoXQgEAAiBAAAWymA4ipVWivVu+8r7zAQCACKEQAAAC2UwHMVmrxMppSvvm4VAOec5dgYA\nzJsQCABggQx2R5+3FCwiot/rxGiSYziazKkrAKAOhEAAAAtkMBzF5vrnh0CzUGhr15IwAFhmQiAA\ngAUyWw52tdnb9gUCgOUmBAIAWCDXDIHWhUAAgBAIAGChfKGZQJaDAcByEwIBACyI6TTH9t71Q6Bt\nM4EAYKkJgQAAFsTO3jhyjpeeDrbejQjLwQBg2QmBAAAWxCzkeXEIdK7bjnYrxdbwYB5tAQA1IQQC\nAFgQ1wuBUkqx2euYCQQAS04IBACwIK4XAkVE9HudGAzHZ90SAFAjQiAAgAVxJQRaf2kItNHrxNau\n5WAAsMyEQAAAC+ILzQTa7HWcDgYAS04IBACwIGYbP18vBLInEAAsNyEQAMCCGAxH0W23otdpv+Rj\n/fVObAmBAGCpCYEAABbE9nAUG71OpJRe8rHZcrDpNM+hMwCgDoRAAAALYjAcxWZv5Zof2+x1Ypoj\nLh04IQwAlpUQCABgQRyGQC/dDyjihX2CBruWhAHAshICAQAsiK3dCiGQfYEAYGkJgQAAFkSlmUBC\nIABYWkIgAIAFMRiOor/evebHZu/fshwMAJaWEAgAYAFMpjl29saxYSYQAHAdQiAAgAWws3cY7lgO\nBgBcjxAIAGABzMKd64VAa51WdFdasTU8OMu2AIAaEQIBACyA2V4/1wuBUkqx2evEtplAALC0hEAA\nAAtgNhOov37tECjiMCCyHAwAlpcQCABgAdxoOVhERL/XcToYACwxIRAAwAKoEgKZCQQAy00IBACw\nAIRAAMCNCIEAABbA9nAU3ZVWrHXa163ZXO/EwHIwAFhaQiAAgAWwtTv6grOAIg5nAu3sj2MyzWfU\nFQBQJ0IgAIAFMBiOol8hBIoIx8QDwJISAgEALIDB8MYzgWbHx28JgQBgKQmBAAAWQJUQaPZxm0MD\nwHISAgEALAAhEABwI0IgAIAFsD0cxcYNQ6BuRERs7R6cRUsAQM0IgQAAGm48mcbO/rjyTCAbQwPA\nchICAQA03PbeOCJe2Pj5eiwHA4DlJgQCAGi4Wahzo5lA3ZVWrHfbsbUrBAKAZSQEAgBouKoh0KzG\nTCAAWE5CIACAhhMCAQBVCIEAABpudtpX1RBoSwgEAEtJCAQA0HCz0742b7AxdMRhCOR0MABYTkIg\nAICGsxwMAKhCCAQA0HCD4SjWOq1YXWnfsLa/LgQCgGUlBAIAaLjBcFRpFlDE4Uyg3YNJHIynp9wV\nAFA3K2d5Y920mtfi3FneJEAj7MXlOMj7ad59AM1UGgLNPueOC6un2RacuG7nXF5b7VeuT9NcuTa3\ny34N51RW39orm4GXOzee2fdCbeHf9gsz4OLeV6tfZqb9wpmJ7er3S0REbpXdN2la/c7J7dKxqz8e\njyO3qj8m02hSNnjh11r68xEl5QVfZ0REOhgX1Zf87EVE0c9TmpTd79Nu9Z+lvb3nY3Rw+YZ3zpmG\nQGtxLr4yfd1Z3iRAI/xSfs+8WwAabGu3IARa70aEEIhmWlvtx1f+jj9Tub4kvBjf0ivqZbpSdlG8\n9okniuond99SuXZ4z3rR2Cu7ZReiax9/vKj+4ME7K9d2P1V2v+RbN4vqp0fPeVW1Lu9Xrh1vlj1m\n2kcnOZ6WaUH4tvL0dtnYG2WPsVwQXkSUhbCTtbKxu48+X1Q/uqd60BwR0d6rHjK1n90pGnvvgdsq\n1z70yz9Uqe6mloOllN6UUvpESumTKaXvupmxAAA4nsOZQNUudF6YCXS6FyNQhesJgLN17BAopdSO\niLdHxDdGxOsj4ltTSq8/qcYAAKhm+5jLwWCeXE8AnL2bmQn0xoj4ZM750znng4j4yYj4ppNpCwCA\nqkr2BOoLgagP1xMAZ+xmQqB7I+KRq95+9Oh9AACckdFkGpcPJsUzgbZ2hUDMnesJgDN2MxtDX2vn\nppdsd55SektEvCUiYi3KNpMCAOAL2z6a0bPZq/aybsNMIOqj/HqiW7YpMACf72ZmAj0aEfdf9fZ9\nEfG5FxflnN+Rc76Yc77YCSdQACyKqpt5ppT+cEopp5QunmV/sCy2ZiHQerWZQO1WigtrK0Ig6qD8\neqJz7syaA1hENxMCvT8iXp1SejCl1I2Ib4mId51MWwDUWdXNPFNKFyLibRHxS2fbISyPWZjTr3g6\nWMThkrCB5WDMn+sJgDN27BAo5zyOiLdGxE9HxMci4p/lnD96Uo0BUGtVN/P86xHxfRGxd5bNwTKZ\nhUAbFfcEijgKgcwEYs5cTwCcvZvZEyhyzu+OiHefUC8ANMe1NvP8yqsLUkpfGhH355z/fUrpL59l\nc7BMXtgTqHoI1F8XAlEPricAztZNhUAALK0vuJlnSqkVET8QEd9+w4Gu2vDz5S9/+Qm1B8tjcIwQ\naLPXid/YvnRaLcGpye0U+7etVa5fe6b62O3tg6JeOpfLJrlO7uoX1R/cWv3rXNmdFI3dKfxaY7X6\nctOIiNZB9X4uXSz73X/ufb9ZVD95bdmBc8OC79P5hz5bNHa+ZaOoPkbjovLUPV+5dnquVzT2tOB3\nTEREe2e/qD532pVrR+fLejm475ai+u7j20X1k1uqH4A1ubX69ygiYvWR5yvXtg6qPV5uZk8gAJbX\njTbzvBARvyMi/lNK6TMR8VUR8a5rbQ599Yafd9xxxym2DItpdtR7aQhkJhAALB8hEADH8QU388w5\nD3LOt+ecH8g5PxAR74uIN+ecH5pPu7C4BsNRrHfb0V2p/rJus9eNwXAUOb/kNG4AYIEJgQAodr3N\nPFNK35NSevN8u4PlMhiOimYBRRzOBDoYT2NvND2lrgCAOrInEADHcq3NPHPO332d2t91Fj3BMjpu\nCDT73F63+j4MAECzmQkEANBgg+Go6Hj4iMPTwWafCwAsDyEQAECDbd/ETKCt3cITggCARhMCAQA0\n2NbuzS0HAwCWhxAIAKDBBsNR9IVAAEAFQiAAgIY6GE9jOJqUzwSyJxAALCUhEABAQ81CnFmoU9X5\n7kq0khAIAJaNI+IBABrqSghUOBOo1Uqx0esIgWikNM2Va1uX9irXjm89V9ZI4Z/TWzvVe4mIWCuo\nv/T628p62W8X1ec7N4rqo+B7lAvvx+GXvaKo/tLLyp4f17amlWufftMri8bevyUV1a/sVr8fIyJS\n9daL7/fO5bJeNj5V9rXmdvX61qSsl9VPPllUP7r/9qL61mhSvbbw925e7RY0Uu2baiYQAEBDzUKc\n0iPiIyL6vU5s7QqBAGCZCIEAABpqMDw84r10JtDsc8wEAoDlIgQCAGioWYhTejpYRFgOBgBLSAgE\nANBQg93j7QkUEdFf7wqBAGDJCIEAABpqMBxHxPH2BNrsrQiBAGDJCIEAABpqMBzFuW47Ou3yl3Sz\nPYFyLjtlBQBoLiEQAEBDDYajYy0Fi4jo97oxmea4tD8+4a4AgLoSAgEANNRgeHCspWARL+wjZEkY\nACwPIRAAQEMNhqPorx8vBNoQAgHA0hECAQA01E0tBzsKj2YnjAEAi08IBADQUDcTAlkOBgDLZ2Xe\nDQAAcDxCIJZNmuboXCrYzDylyqWTtXZRL+0Pf6qofv+NrymqX318p3Lt+Y88XTT23gO3FtVPc9l9\nM16vXj9dqf49iojYva/sOS8XXvE+/9rqve/fUna64nR1Wla/VlafxtXvy9awbD7I2rNl9ZPOelH9\n+tOTyrWtceH9fuuFovr2zn5RfXr8qeq1vV7R2Huvv6dy7fSz1b5HZgIBADTQ3mgSe6PpTS8H2xIC\nAcDSEAIBADTQ9lF4s7nePdbn9zrt6LSTmUAAsESEQAAADTQLb447EyilFJu9jhAIAJaIEAgAoIFu\nNgSafa7TwQBgeQiBAAAa6MRCIDOBAGBpCIEAABpICAQAlBICAQA00NbuzYdA/fVubA0PTqolAKDm\nhEAAAA00m8GzsbZy7DHsCQQAy0UIBADQQIPhKC6srsRK+/gv5zZ6ndjZH8dkmk+wMwCgroRAAAAN\ntD0cxcZNLAWLiOj3OpFzxM6e2UAAsAyEQAAADTQYjm5qP6CIF/YTsjk0ACyH4y8ih2PY/8avKKp/\n5jt2T6mTiPjlzaLye7/3vafUCACUEwKxjHIrxeh8ySXMeuXKlZ2yTdJbt91SVL/6xKWi+txpV68t\nfC5Ye7ysl9EtvaL64f3d6rV3pqKx23tF5TG8s2y568Fd1Z8Puxv7RWPfcn5YVJ9z2X0zPKj+OLj8\nXNn3dK9VFh10B4W931b98X7rR7aLxj64/VxRffe5su9T2tyoXJs7ZffjyuVx9T4qLu02EwgAoIG2\nTiAE6q8ffv6WzaEBYCkIgQAAGmgwHF0JcY7LTCAAWC5CIACABrIcDAAoJQQCAGiYvdEkDsbTmz4d\nbEMIBABLRQgEANAws9DmZmcCrXXasdZpCYEAYEkIgQAAGma2kfPNhkCzMQY2hgaApSAEAgBomNnM\nnZvdGDoiot/rxtaw7GhsAKCZhEAAAA1zUsvBZmNYDgYAy0EIBADQMCcZAm30OjEYjm96HACg/oRA\nAAANc5IhUH+9E4Ndy8EAYBkIgQAAGmYWAl1YsxwMAKhuZd4NUD/tVz1Yufbh77i7aOxf/xM/VFQ/\njWlR/Xc/9RWVa9/7j76yaGwAqIvB7kFcWFuJdivd9FibvU5cPpjEaDKNTtvfB6m31sEkeo/uVK6f\nrncr1+Z22c/T7m+7q6i++9xeUX374Ucr105ffV/R2Hv3XiiqT+NcVD8tyKfTpGjouHxfWS+TO8tm\nOm70dyvX3rs5KBp7rV0WuLdS2df68LN3VB97reyOX3ms+s9SRMSo7CEW3e3q133jjdWisVuTsmvK\ntLtfVJ9X2pVrJ5u9orE7n326cm3ar7a02296AICGGQxHJ3IyWMQLJ4yZDQQAi08IBADQMIPh6ET2\nA4p4YV8hIRAALL6bWg6WUvpMROxExCQixjnniyfRFAAA13eSIdCGEIg5cj0BcLZOYk+g351zfuYE\nxgEAoILBcBR3b66dyFh9IRDz53oC4IxYDgYA0DCD4fjkl4PtCoEAYNHdbAiUI+I/ppQ+kFJ6y7UK\nUkpvSSk9lFJ6aBRlu2wDAPD5cs4xGB5cWcZ1s+wJxJwVXU8cTKqf3ATAS93scrCvyTl/LqV0Z0T8\nTErp4znnn7+6IOf8joh4R0TERrq17Iw7AAA+z3A0idEkR79Xdlzv9QiBmLOi64nN3j2uJwBuwk3N\nBMo5f+7o/09FxL+OiDeeRFMAAFzbLKw5qeVgK+1WnF9diS3LwZgD1xMAZ+vYIVBK6VxK6cLs3xHx\neyLiIyfVGAAAL3XSIdBsLDOBOGuuJwDO3s0sB7srIv51Smk2zj/JOf/UiXQFAMA1zTZwPskQaEMI\nxHy4ngA4Y8cOgXLOn46IN5xgL5ySz/3PX11U/9ff8g8r137j+vNFY/+1p768qP6nfuRriurv/uef\nqFy7/uwvFY0NAHVwGjOB+r1ODIYHJzYeVHGc64lppx1795yvXN/dqn4wzf5tayWtRO/RS0X1+3ed\nK6qfvOHByrWDV64Wjb06mBbV791StoBkeHuqXLtfuG3s5LaywLp/y+Wi+lf0q1/f3LJatlH5cFL2\nvP3E5Y2i+tG4Xbk2P1+2r1x7r6g8Vp8v+762C76te7eW3Y+rW+Oi+um5sueC1u7pHYA1vX2zevEz\n1b7/jogHAGiQLcvBAIBjEgIBADTI9iwEWhcCAQBlhEAAAA0yGI4ipYgLqzeztePn6693nA4GAEtA\nCAQA0CCD4Sg21jrRalXfc+NGNnqd2B9PY280ObExAYD6EQIBADTIYDg60f2AIl7YX2jbkjAAWGhC\nIACABtnaPfkQqH+0v9CWEAgAFpoQCACgQQbD0ZXQ5qTMQiWbQwPAYhMCAQA0yPZwFBuntBxsYHNo\nAFhoQiAAgAY5jT2B+r1uRFgOBgCLTggEANAQOedT3RjacjAAWGxCIACAhtg9mMR4mk88BLqwthIp\nCYEAYNGtzLsByuX/6g1F9e95298sqt9sdSvX/pUnvrJo7I993YWi+ju2frGoflJUDQDNMluuddIh\nUKuVYmOtE4PdgxMdF05aujSM7v/3a5XrJ1/92yvXdp8re/y3nnq+qH7l/GpZ/ZOD6rUvu6to7PZ+\nLqqfrKai+tGF6uNP+uOisTduvVxU/4p+2ffpZevV7/dJLrtfnt0/V1T/5KDs2ml/t/rvhvblsvkg\nK8Oi8uLHzN4t1WsvPFJ21TddKXz83rpWVN85xak1k4KDIHKr2tdpJhAAQEPMNm7un3AIFHEYLJkJ\nBACLTQgEANAQg1OaCTQbUwgEAItNCAQA0BCzkOakj4iPiOivd5wOBgALTggEANAQ26c4E2jDTCAA\nWHhCIACAhriyHKxgo8iqNnudKyETALCYhEAAAA2xNTyIVoo43z35A177vU5s7Y4i57JTgwCA5hAC\nAQA0xGA4is1eJ1oVj4EtsdnrxHiaY/eg7OhdAKA5hEAAAA0xGI5PZT+giBf2GbIvEAAsLiEQAEBD\nzGYCnYb+0T5DW7tCIABYVEIgAICGGAxHp3I8fMQLx86bCQQAi0sIBADQEIPdA8vBAIBjO/mjJSiW\nVsq+DYO/ermo/pbWWlF9iY//ofuK6idbv3VKnQDA4hsMR1eWbZ20/nr36DYOTmV8OAlptRutVz1Y\nuT4Px5Vr9+8oe828Mtgoqj/YLPvZHZ+7rXLt6lbZhu7bLy+7/hjeVXZq4Pj8tHJt+1xZ8LzZ2yuq\nv6u3XVS/kqrfl88dXCga+7PP31JUv/dMr6i++1y7cu3qs2UHDIzXi8qj93TZY6a7Xb2+s132mOk+\n9nxR/e5r7iirv+9c5dre48OisVsFz2FR8XRPM4EAABog5xzbezaGBgCOTwgEANAAl/bHMZnmUwuB\nznXb0W4lIRAALDAhEABAA8zCmdMKgVJK0e91nA4GAAtMCAQA0ACnHQLNxjYTCAAWlxAIAKABBkcz\ndE7riPjZ2EIgAFhcQiAAgAaYhTP9XvfUbqO/LgQCgEUmBAIAaIAry8FO6Yj4CMvBAGDRCYEAABrA\nnkAAwM0SAgEANMBgOIp2K8W5bvvUbqN/FAJNp/nUbgMAmB8hEABAAwyGo9jsdSKldGq3sdHrRM4R\nO/vjU7sNAGB+hEAAAA2wdRQCnabZ+NuWhAHAQlqZdwNEfO5tbyyqf/8bfrCoflpUHfG1v/bfVa7d\nePrJwtEBgOPYPoMQqL9+ePLY1u4o7r/1VG8KjmcyjbS1U7m8vb9aubbTK7s0mp6rPnZERGtctswy\nF0z662yXBbe5XfZc0irMhXOv+hXI7f1LRWPfd36rqL7XLmt+PK2+5PaRnX7R2NvPrxfVd54vW/7b\n2q/+oMmF00FGG6XLhMtmra7sVh9/5xVrRWNPv+hlRfX9h3eL6tOk+nPHtHBJdy45DKJd7ZtqJhAA\nQAMMznAmkM2hAWAxCYEAABpACAQA3CwhEABAA5xFCNQ/mna+NTw41dsBAOZDCAQAUHPTaT6TPYHM\nBAKAxSYEAgCouZ39cUzzCzN1Tstapx3dlZYQCAAWlBAIAKDmZke2b5zyTKCIiH6vE4NdIRAALCIh\nEABAzc1m5pz2crDZbZgJBACLSQgEAFBzQiAA4CQIgQAAau4sQ6D+eie2LAcDgIUkBAIAqLlZKHMW\nIdCGmUAAsLBW5t0AETuvGc+7hc9z7q9vVK6dXv70KXYCAES8MBPotE8HizgMmraFQNRU7q7E6OV3\nVK5feWancm33t54rbCYXlfce2S+qH73y7sq12w/2isYudbA5LarvnD+oXHtrb7do7Ht7W0X16+3q\nvUREfHDr/sq1jz/ZLxq7/Uy3qH66WvYYK5Hbqah+UthLJ5eNPzpfvb5V+Cuqe6mw9ycGRfUrg+3K\ntdMHX1Y0dhpX/9mrWmsmEABAzQ2Go+i0U/Q67VO/rX6vGzv74xhPyi76AID6EwIBANTcYDiKzV4n\nUir7y+pxbPYOJ4pv79VrpjIAcPNuGAKllN6ZUnoqpfSRq953a0rpZ1JKDx/9/5bTbRMAYHltD0ex\ncQb7AUVEbB4tObMvECfF9QRAfVSZCfSjEfGmF73vuyLiPTnnV0fEe47eBgDgFMxmAp2F2e1s7Zbt\nowFfwI+G6wmAWrhhCJRz/vmIePEuad8UET929O8fi4g/eMJ9AQBwZGt4cIYh0OHGpWYCcVJcTwDU\nx3H3BLor5/x4RMTR/+88uZYAALjaYDiK/hnPBBICccpcTwDMwalvDJ1SektK6aGU0kOjKDsWEYD6\nSim9KaX0iZTSJ1NKL5nGn1L6symlX0spfSil9AsppdfPo09YBIPds18O5ph46uLzridGl+fdDkCj\nHTcEejKldE9ExNH/n7peYc75HTnniznni51YPebNAVAnKaV2RLw9Ir4xIl4fEd96jZDnn+Scf2fO\n+Usi4vsi4vvPuE1YCNNpjp398Rz2BBICcaqOdz3ROXdmDQIsouOGQO+KiG87+ve3RcS/PZl2AGiI\nN0bEJ3POn845H0TET8bh/g5X5Jy3r3rzXETkM+wPFsbO3jhyjjM7Hay70or1bttyME6b6wmAOahy\nRPxPRMQvRsRrU0qPppT+dER8b0R8Q0rp4Yj4hqO3AVge90bEI1e9/ejR+z5PSunPp5Q+FYczgd52\nrYGunub/9NNPn0qz0GSzMOasZgLNbksIxElxPQFQHys3Ksg5f+t1PvR1J9wLAM2RrvG+l8z0yTm/\nPSLenlL6YxHxv8cLf/W9uuYdEfGOiIiLFy+aLQQvsjU8PKr9rEOgLSEQJ8T1BEB93DAE4vS99jWP\nner4b3vsvymqb3/4U5Vrp6XNAIvi0Yi4/6q374uIz32B+p+MiB8+1Y5gQc1m5PTXu2d2m2YCUVs5\nojWu/gp0fMeForFLpFz2Ce2Pf7ao/tL9vcq1k9Vr/W3m+vb7ReUx7ZV9rbdvVt/A+1UXymYB39Md\nFNU/VvjFPnGp+mMmD8sup3On8EFWeLE1un1c0EzZ2KX2byurb42r71STJmVjr22V3ZGXfnvZYYUr\nu9W/2NWwX9srAAAgAElEQVTPPlc09vSW85Vrc6r2PHDqp4MBsJDeHxGvTik9mFLqRsS3xOH+Dlek\nlF591Zu/LyIePsP+YGHMazmY08EAYPGYCQRAsZzzOKX01oj46YhoR8Q7c84fTSl9T0Q8lHN+V0S8\nNaX09RExiojn4xpLwYAbm0cI1F/vxIcfFQIBwKIRAgFwLDnnd0fEu1/0vu++6t9/4cybggVkY2gA\n4KRYDgYAUGOD4Si67Vasdc7uZdtmrxPD0ST2x4UbLwAAtSYEAgCoscHuKDbXO5Eqbvh4EjaPNqE2\nGwgAFosQCACgxgbD0ZkuBYt4YemZzaEBYLEIgQAAamyeIZCZQACwWIRAAAA1No8QqH90e1u7QiAA\nWCRCIACAGjMTCAA4KUIgAIAaG+wKgQCAkyEEAgCoqck0x87++MxDoA3LwQBgIa3MuwEi/t1r31VU\nPy0c//HhZtn4O08U3gKL5us/slNUf3/nucq1/9cPf2vR2Hf/3+8tqgdYJLPTuc46BGq3UlxYWzET\niNqZdlPs3turXN/ZmVSuXX287PVPem5QVH/wO19ZVL/xG9X7ufzg+aKxB68qmwuQNg6K6u8+V733\nu7vbRWOXevqg7L7Z3e9WL065aOzpZtlz6oVbdsvq1/aL6ksU3S8RsXNhrah+mKrX954se/zub5TV\nd3fKrrjXf/Ny5drRPf2isTtPVH+eSZNqz3dmAgEA1NRgTiHQ7DYdEQ8Ai0UIBABQU/MMgfrrndgS\nAgHAQhECAQDU1JUQaH0+M4EsBwOAxSIEAgCoqa05LwcTAgHAYhECAQDU1CyE6c8lBOo6HQwAFowQ\nCACgpmYbM2/McWPonMtOvwEA6ksIBABQU4PhKFZXWrHWaZ/5bW/2OnEwmcbeqOyoXACgvoRAAAA1\nNdgdzWU/oIjD08EiIraGB3O5fQDg5AmBAABqajCcXwg0u12bQwPA4hACAQDU1Nbw4MqMnLN2JQSy\nOTQALAwhEABATQ2G47nPBNoyEwgAFsbKvBsgopPKNnscFR7S0QqnelzL+Gu/vKj+03+0cFPOVHa/\n/9df/InKtb//tl8tGvubzz1XVF+q5DF8/9t+qGjs7zj31qL6B97xcFH95Omni+oBztL2cBSvu+fC\nXG7bcjDqKUVupcrVq49sVa7dv79f1EmnMKDNK9X7joiIlep/r9/rF/5tv7C8d65sb7A71i5Vrr3Q\n3isae3faLap/bv9cUf10Wv371L97p2jsB295tqj+5eeeL6pfb1X/Po1y2bXN43ubRfWP9sp+nn5r\nfGvl2oPhatHYq4WXQq1J2XXc+LZe5dqVwX7Z2HdUfw2QH6v2PTUTCACgpua6J9DRMrRtIRAALAwh\nEABADY0m07i0P7/lYBdWV6LdSrFlTyAAWBhCIACAGprNwJlXCJRSio21FcvBAGCBCIEAAGpoFr7M\n63SwiMMASggEAItDCAQAUEODOc8EiojYXO86HQwAFogQCACghmoRApkJBAALRQgEAFBDdQmBnA4G\nAItDCAQAUEOz8GVjjiFQv9eJrd2Dud0+AHCyhEAAADU0O5p97jOB9saRc55bDwDAyRECAQDU0GA4\nil6nHasr7bn1sNnrxGSa49L+eG49AAAnZ2XeDRDxX/amRfUXV8vqX7Y+KKr/zQsXKtdOd3aKxj5t\nl//QV1aufd3/8pGisd99338qqm8VZqzTqP59/ey4bGr+33jmK4rqS11o71Wu/fO3fKJo7A/+jz9Y\nVP9P/+Q9RfU/8S2/p3Lt9EO/XjQ2wM0YDEdznQUUEbF5dDz91u4oLqzNtxeIiGhf3o+N936mcv3k\nntsr1659/PGiXra++v6i+takqDyGt1f/mRudT0VjjzbKgt0714dF9Q/2nqlc+7LO80Vjf3hYdr+P\np2Wvye/arH5982W3PlI09it7TxfV31t433zmoPrjfX9a9pw+ymV/kNgerRXVr/aq7z+3f6FbNPak\nWzj3pXDya+eZ3cq1+3efLxo7TQuaaVV7HjATCACghmoRAh3dvhPCAGAxCIEAAGqoTiGQE8IAYDEI\ngQAAamgwHM31ZLCIiP5sOZgQCAAWghAIAKCG6jQTyHIwAFgMQiAAgBoaDEdXZuLMixAIABaLEAgA\noGZGk2nsHkzmPhOo12lHt92KrV0hEAAsAiEQAEDNzGbezDsESinFRq9jJhAALAghEABAzdQlBDrs\nYcXpYACwIIRAAAA1U68QqBNbw4N5twEAnAAhEABAzQyO9uDZnPPG0BER/fWu5WAAsCCEQAAANVO3\nmUBCIABYDCvzboCIP/Ef/2xR/cf/wNuL6n/gZf+5qP4PfPF3VK5N/+VDRWO31teL6p/477+kqP5H\n/tIPVq59Q7do6GK/+9f+SFF9fuedlWt7T5dNy2//3K8U1Zdqrfcr1/6jP/WmorF/z//w3qL677nz\n/UX1f/f/HFeuvfX3Fw0NcGx1C4GcDkZd5G4nRq+8u3L9eL365c7qaKOol/77Hiuq3754b1F97+nq\nP3c7L28XjR2prPzO9Z2i+rs6g7IbKNCKXFR/6+puWX23ev0Da88WjX2utV9U//D+XUX1j+3fUrl2\nOCn7/bLaqv6aOSLijtVLRfWP96r//O2vlF1TjsvKozUqe4yN+73qtb2yn9XzH3y0cm3aq/acYSYQ\nAEDN1C0E2tkbx2Ra9qIYAKgfIRAAQM0MhqNY77aj057/S7VZELVT8S+MAEB93fCVRUrpnSmlp1JK\nH7nqff9HSumxlNKHjv77vafbJgDA8tjaHdViFlDECyGQJWEcl+sJgPqo8uelH42Ia23i8QM55y85\n+u/dJ9sWAMDyGgzrEwL1j04oszk0N+FHw/UEQC3cMATKOf98RDx3Br0AABAR2zUKgWZ9CIE4LtcT\nAPVxMwvN35pS+vDR9M7q25ADAPAF1Wkm0JXlYEIgTp7rCYAzdtwQ6Icj4osi4ksi4vGI+NvXK0wp\nvSWl9FBK6aFRlB2JBwCwjGoVAlkOxuk43vXE+PJZ9QewkI4VAuWcn8w5T3LO04j4exHxxi9Q+46c\n88Wc88VOrB63TwCApVGrEOioj20hECfo2NcTK+fOrkmABXSsECildM9Vb35zRHzkerUAAFS3P57E\ncDSpTQi0utKOtU7LTCBOlOsJgPlYuVFBSuknIuJ3RcTtKaVHI+KvRcTvSil9SUTkiPhMRPyZU+wR\nAGBpzMKW2alcddDvdWNr92DebdBQricA6uOGIVDO+Vuv8e5/cAq9AAAsvdmyq42azASKOFwSZiYQ\nx+V6AqA+bhgCcfpe/Q8LN8z+A6fTx8zuX92uXHvhj1woGrt1+61F9e/7X3+wqL7ET+zcW1T/Y9/5\nTUX1537q/UX1EZ8urK+P6e5u5do73/7eorF/7SfLHjP/z39+oKj+jz7wgcq1Pxtlj3eA45iFLXVZ\nDhYhBKI+0mQaK89Xf93R+Wz1jaSnd/SLetn/ojuL6le3yn6GRhdO8VJtdVpU3m1Piup3pmuVa29b\nuVQ09nq77NrpzrWdovrzBePvTKp/nRERn9q7o6j+0d2yx+Sze9X3zLpnvfo1X0REf/35ovqdcdl9\nM54U7FRT9vCNaeGv0/Ze2Q3klVS59vyHHisbe6NgH7Rnqt2HN3NEPAAAJ6yWIdB6J7Z2hUAA0HRC\nIACAGqllCNTrOB0MABaAEAgAoEZmM2766905d/ICy8EAYDEIgQAAamQWtmys1Wfrxn6vE5cPJjGa\nFG7EAADUihAIAKBGBsNRnF9diZV2fV6mbR4dV282EAA0W31eXQAAEIPhqFb7AUW8sD+REAgAmk0I\nBABQI9vDUWzUNARyQhgANJsQCACgRrZ2R7HZq89+QBEvhEBOCAOAZhMCAQDUyGA4in6vPieDRVgO\nBgCLQggEAFAjddwTaHZc/dbuwZw7AQBuhhAIAKBGBsPRldO46mJ2XP1gOJ5zJwDAzajXgvMl1frl\njxbVf/GPvq2o/sPf/neK6t/zO/9p5do3/5tvLhr7Nz58T1H9afq+f/yHi+rv/6n3nlIny2Xwx7+q\nqP6pbyhbevDjF/59Uf2X/b/fWbn2NfHLRWMDlNobTWJ/PK3dTKCVdivOr65YDsbc5XYrJpu96vW3\nnqtc297ZK+pl9eEniuonL7utqH73rurLQlNpPjtJZb2My5aorrdOb9Zg6dirrbI759JktXLt0wfn\ni8Z+5PItRfWD/bWi+s3V6o/he9YGRWNfaJf9fDyeN4rq9w6q/95rXy6by9J7JhfVt/cnRfWj89Vj\nlc5e2f04vaNfuTa3qv1cmwkEAFATs42X63Y6WMThvkBbQ8vBAKDJhEAAADWxdRQC1W0mUMRhT04H\nA4BmEwIBANTEbLlVv6YhkOVgANBsQiAAgJoY7NZ3JlB/vRNbu0IgAGgyIRAAQE0Mar4czEwgAGg2\nIRAAQE0IgQCA0yQEAgCoiUGdTwdb78T+eBp7o7KjcwGA+hACAQDUxGA4igtrK9FupXm38hKz2Ulm\nAwFAcwmBAABqYjAc1XIpWIQQCAAWgRAIAKAm6hwC9XvdiAgnhAFAg63MuwEi8nhcVP+qdzxaVP+6\ne/9sUf0Hv/6HKtf++9/2b4vG7ryuXVQ/yqeXU/7qn/u7RfUXL/9PRfX3/ZvHiurr5ONvu6fsE27f\nrz721769sJsyb3nkvy2qf913faJyrV0wgNNW5xDITCBqYXcv0ger/+5uve6LKtfmbuGlUbfsZ3V4\nd6+oftQrWBZauII07Ze9xh6Oy77W3Wm3cu0d7e2isbcna0X1L199tqj+qdFG5drBqOx7utHdK6p/\n1YWni+pfu/5E5doHumVjP7x/d1H99kHZfbP3XPXv6/lnyh7wa8+XvYpvXz4oqt+/pfrjPd93V9HY\nKeeC2mp1ZgIBANSEEAgAOE1CIACAmqh1CLR+2NfWbtlfSAGA+hACAQDUQM45BrujK2FL3VxYXYmU\nIrbNBAKAxhICAQDUwN5oGgeTaW1nArVaKTbWOpaDAUCDCYEAAGpgFq7UNQSKiOivd2JLCAQAjSUE\nAgCogSaEQJs9M4EAoMmEQAAANSAEAgBOmxAIAKAGZqdu1T4E2hUCAUBTCYEAAGpgNsOm3+vOuZPr\nMxMIAJpNCAQAUANNWg6Wc553KwDAMQiBAABqYHs4ipQiLqytzLuV69rsdWI8zXH5YDLvVgCAY6jv\nqwyua/zZR4rqX/3tZfVf8Tf+YuXav/NH31k09tf1dovqpzEtqj9Nv/yXf7CovvWXyzLWOn2tpVoF\nefK7Lt9SNPZ3/Zs/XlT/mu//dFH9ZOvJonqA0zIYjuLC6kq0WmnerVxXf/1wltJgOIrzq15GcvZS\nqxWt9fXK9eP16jPrVp69XNTL5NbzZfWrZT/bm5/Zr1y7c99q0diX7yvr5cmdsq/18Vv7lWsnvbLX\nzF+7/pmi+p1p2fhPTqp/rc/2yu6Xdip7vf/AyrNF9be2qy/X/dD+nUVjf/TSvUX1Dz9ze1H9yqD6\n75RW4ark1efGRfV5pewxc+43qn+fRndtFI3d+dhvVS8eVbtjzAQCAKiBwXAUmwUXrPMwW6pmc2gA\naCYhEABADWwNR7XeDygiYuOov63hwZw7AQCOQwgEAFADg+Go1ieDRbxwctm2E8IAoJGEQAAANTBo\nwEygzav2BAIAmkcIBABQA9vD0ZXlVnU1C6m27AkEAI0kBAIAmLOccyNmAp3rtmOllcwEAoCGEgIB\nAMzZcDSJ0STXPgRKKcVmryMEAoCGEgIBAMzZbHlVv+ZHxEccLgnbEgIBQCMJgQAA5mw2s6buM4Ei\nDjeHdjoYADSTEAgAYM4aFQJZDgYAjSUEAgCYs6aFQE4HA4BmWpl3A9TPg3/lFyvX/p0f/4NFY3/3\nV9xaVH/3n/rNovoSf+uBf1lU/4qV7il1Uu6D+2X57d945PedUieHPva+ByvXvuZHHi8a+5Wfrv54\njIgYF1UD1EOTQqC+mUDM00o74rZ+5fL2r36ycm3+ovuLWmntHhTVr1yeltVfqv5z1hqXvU5dfa7s\nteTO4xeK6h++9Y7KtVvn14vGXutcLqqPVtn9/rpuSf12WS+FLk1zUf3PDu+uXPvPnvqKorE//OTL\niupHD28U1fd/o3ptd2dSNHbKZfdja3tYVD+6q+BrLZyGM31F9e9pvlztNYSZQAAAczbbY2ejASHQ\nZq8T23ujmBZenAAA8ycEAgCYs63dUbRSxIXV+k/S3uh1IueInT1zLwGgaW4YAqWU7k8p/VxK6WMp\npY+mlP7C0ftvTSn9TErp4aP/33L67QIALJ7BcBQbvU60WmnerdxQf/1w2YklYVTlegKgPqrMBBpH\nxF/KOb8uIr4qIv58Sun1EfFdEfGenPOrI+I9R28DsCRSSm9KKX0ipfTJlNJLfgeklP5iSunXU0of\nTim9J6X0inn0CU0wGI4asR9QxAv7FgmBKOB6AqAmbhgC5Zwfzzn/ytG/dyLiYxFxb0R8U0T82FHZ\nj0VE2Q7BADRWSqkdEW+PiG+MiNdHxLcevaC/2gcj4mLO+Ysj4l9ExPedbZfQHE0MgbaGZZvisrxc\nTwDUR9GeQCmlByLiSyPilyLirpzz4xGHT+wRced1PuctKaWHUkoPjWL/5roFoC7eGBGfzDl/Oud8\nEBE/GYcv5q/IOf9cznn36M33RcR9Z9wjNEaTQqD+uplAHN/NXk8cTMpO7QHg81UOgVJK5yPiX0bE\nd+acK5+Fl3N+R875Ys75YidWj9MjAPVzb0Q8ctXbjx6973r+dET8h2t94OoX908//fQJtgjNMdsT\nqAksB+O4TuJ6otvunV6DAEugUgiUUurE4RP2j+ec/9XRu59MKd1z9PF7IuKp02kRgBq61u611zwv\nOqX0JyLiYkT8zWt9/OoX93fccccJtgjNMRiOot+wEGhrVwhEda4nAOqhyulgKSL+QUR8LOf8/Vd9\n6F0R8W1H//62iPi3J98eADX1aETcf9Xb90XE515clFL6+oj43yLizTlna4LhGnLOjVoOttZpx+pK\nK7bNBKIi1xMA9bFSoeZrIuJPRsSvpZQ+dPS+vxIR3xsR/yyl9Kcj4rci4o+cTosA1ND7I+LVKaUH\nI+KxiPiWiPhjVxeklL40In4kIt6Uc/bXXbiOyweTmExzY0KgiMPZQJaDUcD1BEBN3DAEyjn/Qlx7\n2n9ExNedbDsANEHOeZxSemtE/HREtCPinTnnj6aUviciHso5vysOl3+dj4h/fvhH4PitnPOb59Y0\n1NQsTBECsahcTwDUR5WZQHBdk49+oqj+lo+Wjb//o2X1Jd72pW8pqp+ul704z9d7qXMd6Zq7qVxb\ne7BXNPb0Ix8va6bQK+OJyrXjU+yDs5VzfndEvPtF7/vuq/799WfeFDTQYLd5IVB/vWNPIOYit1sx\nvbBeuT6de3n1sVeKDk6O1qWy12O9n//1ovqdN/2OyrWrW5OisbvbZV9rPFJ22fjBter3++VR2eFB\nj9zzq0X1X772maL6Ub5UubZd+Hr/fXtf6AyNl/qNvd9WVP+zT1Sv/+xvlu3DuPpk2WNg8zeLyqP/\ncPWT/9qFf4SYrhVex/3WY0X13WH1HQ9G991W1ssHCi6gc7XnpMKffgAATtLW8CAiIjYL/9gwT2YC\nAUAzCYEAAOZou4HLwTaEQADQSEIgAIA5auKeQP1eVwgEAA0kBAIAmKMmhkCbvU5c2h/HeDKddysA\nQAEhEADAHA2Go2i3Upxfbc55HZu9w16392z3DwBNIgQCAJijwXAUG2srkVLhMTNz1F/vRkTE1u7B\nnDsBAEoIgQAA5mhrd3QlVGmK2dI1+wIBQLMIgQAA5mgwHMVGg/YDiogr/QqBAKBZhEAAAHO0PRw1\nalPoiIj+uhAIAJpICAQAMEeDBoZAloMBQDM15xgKOGH5gx8tqi/drvM0t/d0IC/A4jgMgZr1kuxK\nCLQrBOJspfE02s/vVK7f/rJ7KteuPbVf1MveXXcU1a/efqGo/twju5VrW/tlJ/Xt3tkvql97NhfV\nt0arlWsfvnxv0dh/65PVv6cRESsXyp6n1terPw72D8qeuw92C/d/2y4b/9wj7cq1G4X7+q8Oyh4D\n5x4vu987T1X/ud57ednjN1plV2Zpb6+sfrX6H3LaO2XPM8Pf+xWVa/Mv/GKlOjOBAADmZDrNjZwJ\n1Gm34ly3HVtmAgFAowiBAADm5NLBOKY5ot9r1ulgEYezgSwHA4BmEQIBAMzJbDlV02YCRRyeECYE\nAoBmEQIBAMzJLERp2hHxEYcnhNkTCACaRQgEADAn28PmzgSyHAwAmkcIBAAwJwMhEABwhoRAAABz\nMjtda3O9eSFQf70bW8PCM4YBgLkSAgEAzMlsJk2/oTOB9kbT2B9P5t0KAFCREAgAYE4Gw1GstFKs\nd9vzbqXYbDNrS8IAoDmEQAAAczIYjmKz14mU0rxbKTabveSEMABoDiEQAMCczEKgJto0EwgAGmdl\n3g0AACyr7eHoyrKqphECMRetFLm3Wrl87dnqm5d3P/VEUSvte28vqh9f6BbVr37m2cq1eaVsSenK\nXi6qT9Oy+t5T1etbB2W9T9bK6js7Zc+xk9565dp22bc0zhfupb/+5LSovntpXLn2udeWRQGbv7JX\nVN95aqeo/uBlm5VrV5/eLRo7HVS/XyIi8hteV1Q/Pl/9Oal9ab9o7PXf3Kpc29qvtkefmUAAAHOy\ntTuKfgNPBouIK31vWQ4GAI0hBAIAmBPLwQCAsyQEAgCYkyaHQBfWhEAA0DRCIACAOZhOc2zvNTcE\nardSXFhbEQIBQIMIgQAA5mBnfxw5R2NDoIjDfYGEQADQHEIgAIA52D4KT5p6OljEYYAlBAKA5hAC\nAQDMwexUrX7DQ6Ct3cIzjwGAuRECAQDMwWwGTaOXg/W6ZgIBQIMIgQAA5uBKCLTe3BBoo9eJwXA8\n7zYAgIqEQAAAc7AIM4EO9wQ6iJzzvFsBACoQAgEAzMEihED99U6MJjmGo8m8WwEAKliZdwMAAMto\na3gQnXaKXqc971aObRZgDYajWO96Wcnpm3bbMXz5ZuX69Y8+Xn3wblkgOzlXVr9yqWz/rL1X3l65\nNqeioWPj05eL6sfny77W8X2rlWvXniubSTg6X1Qeowtld07vqer9tMZlva/sldWnaVF5dAfVl+fe\n/pHCXiaFzbTK7vdUMKM0t8rmsqTHnyqrv+uOovo4X/3xnvbKngee/7LqzwPjJ6r9HjYTCABgDraH\no9jsdSOlwqu3GpmFQLOTzgCAehMCAQDMwWA4is1es2fP9K+aCQQA1J8QCABgDg5DoObuBxRxeDpY\nhBAIAJpCCAQAMAeLEAJd2RPIcjAAaAQhEADAHCxCCNRfNxMIAJpECAQAMAdbu6Por3fn3cZNOb+6\nEu1WEgIBQEMIgQAAzthkmmNnb3xlT52mSinFxtpKbA0P5t0KAFCBEAgA4Izt7B3OnGn6crCIiP56\nNwbD8bzbAAAqEAIBAJyx2fKpRQiBNnody8EAoCGEQAAAZ2yRQqDNXicGu5aDAUATCIEAAM7YIoVA\nfTOBAKAxVubdAADAstnaPQxNZkesN9mmEIgz1BqOYv0jn6tcv/XV91euvfCZ3aJeOh9/rKh++txW\nUX3rq3579V6evVw0dtop+1ovX7y3qP72/1z9vhm+6o6isQ82yy5huw+X7Vk26bUr165cnhSN3ZpM\ni+qn7bI5G6sfq36/59ffVzT2ylPbRfVpmovqo+Cu2b9rvWjotenLynpJqai8fbn6bNjBG24vGnvz\nk9V/ttt71e5EM4EAAM7YIs0EmoVA09IX/ADAmbthCJRSuj+l9HMppY+llD6aUvoLR+//P1JKj6WU\nPnT03+89/XYBAJpvkUKg/nonpjni0oETwrg21xMA9VFlLt04Iv5SzvlXUkoXIuIDKaWfOfrYD+Sc\n/9bptQcAsHi2h6PorrRirVN92UFdbRwFWYPdUWysNT/U4lS4ngCoiRuGQDnnxyPi8aN/76SUPhYR\nZYtCAQC4YjAcLcQsoIgXZjMNhqOovvsKy8T1BEB9FO0JlFJ6ICK+NCJ+6ehdb00pfTil9M6U0i0n\n3BsAwELa2l2cEKh/VQgEN+J6AmC+KodAKaXzEfEvI+I7c87bEfHDEfFFEfElcZjs/+3rfN5bUkoP\npZQeGsX+CbQMANBsg+HoSnjSdJvrQiCqOYnriYPp8Mz6BVhElUKglFInDp+wfzzn/K8iInLOT+ac\nJznnaUT8vYh447U+N+f8jpzzxZzzxU6snlTfAACNtYjLwWbH3sO1nNT1RLfVO7umARZQldPBUkT8\ng4j4WM75+696/z1XlX1zRHzk5NsDAFg8ixQC9XvdiDATiOtzPQFQH1VOB/uaiPiTEf9/e/ceI+d1\n3nf898xtZ2YvM0tyI4nkmhJlKzIdkXK6NSgJUdymNWynsVo0AizARnqDA9RunSJA4bpAY/SvNm3T\npIDh1klcB6gdI7AdRA0E24nsIo0vgmldKEuUa1KSyRUpiVxyZi8zu/POzOkf78xyRS+lHWpnzjvv\n+X4AYefycvY58+6S8/70nHP0tJk92Xvsk5IeMrO7JTlJL0r69aFUCAAAkDLLzWhzV61xV8xnVMhm\nCIHwerieAICE2MnuYH8tybZ56pHdLwcAACDd2p2uVjbaqekEMjPNlPKqN1u+S0FCcT0BAMmxk04g\nAAAA7JLl9bYkqVpORwgkxWOhEwij0C3l1XzH/h0fP1Hv7PjY7NLKQLV09s8NdHzj+K0DHT/17KUd\nH7t2576BXnvytBuslhcGe2/W3n7Tjo/NtroDvfbMo88NdHx01+GBjp+4tPPFx21jsL/3on1TAx2f\nX10f6Pjm0fkdH5uJBnvfWwcH27wvVx9sUyjr7vxnMru+899rSVo9PDO0WiQpmtz5puvllwf7mbFo\n52M1t7O6B9oiHgAAAG9OPyxJSyeQFI+FEAgAgOQjBAIAABihtIZA7A4GAEDyEQIBAACMUBpDoCqd\nQKkFxLcAABj7SURBVAAAjAVCIAAAgBFKYwg0QwgEAMBYIAQCAAAYoTSGQJVSXivrbXUGXEwTAACM\nFiEQAADACNUb8VbqMykKgfo7nS3TDQQAQKIRAgEAAIxQvRmpmM+omM/6LmXX9LuamBIGAECyEQIB\nAACMUL0ZpWoqmEQIBADAuCAEAgAAGKE0hkD96WA1QiAAABKNEAgAAGCE0hgC0QkEAMB4IAQCAAAY\noXqznboQaIYQCACAsZDzXQAAAEBI6o2Wjtwy47uMXbXZCdTb+QwYFus6ZZudHR+fHeRnMmoPVIvL\nD/b/03NrO69bkqKbdv73RK4x2Gt3p4qDHT/gWCefOLvjYzfuPDDQa2tu70CHu5wNdHx7qrDjY+vH\nKgO9dvnVwX7G2uXBNhAonVna+cErawO9tk2WBzreTQz2Pzu6xZ1HE7mVjYFeOxN1Bzq+9tbSQMfv\n+97FHR+7fqg60Gu39uy8lm5uZ7+ndAIBAACMUBqng03ksirls3QCAQCQcIRAAAAAIxJ1ulprdVIX\nAklxNxAhEAAAyUYIBAAAMCLLvZCkUkrfjPxqOa9agxAIAIAkIwQCAAAYkX6nTKWcvk6gGTqBAABI\nPEIgAACAEaltdgKlLwRiOhgAAMlHCAQAADAim51ApZ3vPjMuqoRAAAAkHiEQAADAiCzTCQQAADwi\nBAIAABiRespDoEaro1a767sUAABwHYRAAAAAI1JvpDgE6i12TTcQAADJRQgEAAAwIvVmpFI+q0Iu\nfR/B+sEWIRAAAMmVvk8gAAAACVVrRqqmcHt4iRAIAIBxkBvlN1vRlUt/6b78k22e2ifp0ihr8SSU\ncUqMNY1CGafkZ6yHRvz9AHhQb0apnAombQ2BWp4rQZqtrJ6/9M2/+rfJuJ44O9Lv1pe+z2MXrvvM\n7oz1/73pV7i+/7srr5K+c3p94zHWv96VV9l+rM/tymtfz46uJ0YaAjnn5rZ73MxOOOcWRlmLD6GM\nU2KsaRTKOKWwxgpgtOrNSDMpDYGq5XjbezqBMExcT4QxTimcsYYyTomxJgXTwQAAAEZkOYROoAYh\nEAAASUUIBAAAMCJpng42U4wbzGt0AgEAkFhJCYE+67uAEQllnBJjTaNQximFNVYAI5TmECiXzWh6\nIsd0MPgSyr/doYxTCmesoYxTYqyJkIgQyDmX2DdoN4UyTomxplEo45TCGiuA0Wm1u2q0OqqmNASS\npJlSnhAIXoTyb3co45TCGWso45QYa1IkIgQCAABIu344UknpFvFSvC4QawIBAJBchEAAAAAjsBkC\npbgTqFqmEwgAgCTzGgKZ2XvN7EdmdtrMPuGzlmEzsxfN7Gkze9LMTviuZzeZ2efM7FUz++GWx/aY\n2V+Y2Y97X2d91rgbrjPOT5nZS73z+qSZvd9njbvFzObN7FtmdsrMnjGzj/ceT9V5fZ1xpvK8AvCr\nH46kdYt4qdcJRAiEEeJ6Ih24nkjX585QriWk8bye8BYCmVlW0qclvU/SEUkPmdkRX/WMyN9yzt3t\nnFvwXcgu+7yk917z2CckPeqce5ukR3v3x93n9dPjlKT/2juvdzvnHhlxTcPSlvSbzrm3Szou6aO9\n38+0ndfrjVNK53kF4NFyAJ1AlVKe3cEwMlxPpMrnxfVEmj53hnItIY3h9YTPTqB3STrtnHveOdeS\n9CVJD3isBzfIOfdXki5f8/ADkv6od/uPJP39kRY1BNcZZyo55y445x7v3V6RdErSAaXsvL7OOAFg\n19WaLUkpD4GYDobR4noiJbieSJdQriWk8bye8BkCHZB0bsv9RSX8zXqTnKRvmNkPzOwjvosZgZuc\ncxek+BdD0s94rmeYPmZmJ3vtnWPf0ngtM7tV0jslPaYUn9drximl/LwCGL3+gslp3h2sUsqr1e5q\nPer4LgVh4Hoi3VL7uXMbqf3cGcq1hDQ+1xM+QyDb5jE38ipG5z7n3M8rblf9qJnd77sg7IrPSLpd\n0t2SLkj6L37L2V1mNiXpK5J+wzm37LueYdlmnKk+rwD8qDfbktK/JpAk1dghDKPB9QTSILWfO0O5\nlpDG63rCZwi0KGl+y/2Dks57qmXonHPne19flfSnittX0+wVM7tFknpfX/Vcz1A4515xznWcc11J\nv68UnVczyyv+i+wLzrmv9h5O3XndbpxpPq8A/Kk3I00Wsspn07s5a7VUkCSmhGFUuJ5It9R97txO\nWj93hnItIY3f9YTPTyHfl/Q2M7vNzAqSPijpYY/1DI2ZTZrZdP+2pPdI+uHr/6mx97CkX+vd/jVJ\nf+axlqHp/yXW8w+UkvNqZibpDyWdcs79zpanUnVerzfOtJ5XAH7Vm1Gq1wOSrnYCEQJhRLieSLdU\nfe68njR+7gzlWkIaz+uJnK9v7Jxrm9nHJH1dUlbS55xzz/iqZ8hukvSn8c+HcpK+6Jz7mt+Sdo+Z\n/bGkd0vaZ2aLkn5L0n+Q9Cdm9k8lnZX0oL8Kd8d1xvluM7tbcevxi5J+3VuBu+s+SR+W9LSZPdl7\n7JNK33m93jgfSul5BeBRvRmleiqYtHU6WMtzJQgB1xNcT4ybgK4nQrmWkMbwesKcS/O0WQDAOFlY\nWHAnTpzwXQYwFA/+9+8omzF96SP3+C5laM5dbugXfvtb+k+/elQPLsy/8R/AjpnZD1K4LTgAYMTS\nOykdAAAgQUKYDjbDdDAAABKNEAgAAGAEQgiBpidyMiMEAgAgqQiBAAAARiCEECiTMVVKeUIgAAAS\nihAIAABgyDbaHa1H3dSHQJIIgQAASDBCIAAAgCHrhyKhhEC1BiEQAABJRAgEAAAwZPVeKFIpFzxX\nMnx0AgEAkFyEQAAAAEMWWifQMiEQAACJRAgEAAAwZKGFQDVCIAAAEokQCABwQ8zsvWb2IzM7bWaf\n2Ob5+83scTNrm9mv+qgRSIqQQqBqOZ4O5pzzXQoAALgGIRAAYGBmlpX0aUnvk3RE0kNmduSaw85K\n+keSvjja6oDkCSkEqpTy6nSd1lod36UAAIBrEAIBAG7EuySdds4975xrSfqSpAe2HuCce9E5d1JS\n10eBQJL0Q6CZYs5zJcPXD7pqjZbnSgAAwLUIgQAAN+KApHNb7i/2HgOwjVoj0vRETrls+j96VUrx\nDmjsEAYAQPKk/5MIAGAYbJvHbmgBEDP7iJmdMLMTFy9efJNlAcm03Iw0E8BUMOlqJxAhEAAAyUMI\nBAC4EYuS5rfcPyjp/I28kHPus865Befcwtzc3K4UByRNvRkFsR6QtCUEahACAQCQNIRAAIAb8X1J\nbzOz28ysIOmDkh72XBOQWCGFQNUynUAAACQVIRAAYGDOubakj0n6uqRTkv7EOfeMmf17M/uAJJnZ\n3zSzRUkPSvofZvaMv4oBv0IKgZgOBgBAcqV/iwoAwFA45x6R9Mg1j/27Lbe/r3iaGBC8WkAhULmQ\nVS5jqhECAQCQOHQCAQAADFm9GW1Ok0o7M1O1nKcTCACABCIEAgAAGKL1qKNWuxvM7mCSNFMiBAIA\nIIkIgQAAAIaoH4aEMh1MisfK7mAAACQPIRAAAMAQhRgCVekEAgAgkQiBAAAAhijEEKhCCAQAQCIR\nAgEAAAxRrTctKpSFoaU4BKo1Wr7LAAAA1yAEAgAAGKIgO4HKBa1stNXtOt+lAACALQiBAAAAhijI\nEKiUl3PSynrbdykAAGALQiAAAIAh6odA08WwQiBJrAsEAEDCEAIBAAAM0XIz0nQxp2zGfJcyMtVe\nCFRrsi4QAABJQggEAAAwRPVmFNRUMEmqlOkEAgAgiQiBAAAAhqjWaAW1M5jEdDAAAJKKEAgAAGCI\nguwE6k8HaxACAQCQJIRAAAAAQxRyCEQnEAAAyUIIBAAAMET1Zju4EKiYz2oil9EyIRAAAIlCCAQA\nADAkzjktNyPNBBYCSXE3ENPBAABIFkIgAACAIVmPump1usF1AklStZxnOhgAAAlDCAQAADAktWZL\nklQtFTxXMnqVEiEQAABJQwgEAAAwJP0QJMROoEoprxohEAAAiUIIBAAAMCT1RsghUIGFoQEASBhC\nIAAAgCEJvROI6WAAACQLIRAAAMCQhB4CrW60FXW6vksBAAA9hEAAAABDshkClcMLgaq9MTMlDACA\n5CAEAgAAGJJ6M5KZND2R813KyPW7n5gSBgBAchACAQAADEm9GWmmmFcmY75LGbl+CMQOYQAAJAch\nEAAAwJDUm1GQ6wFJV6fA0QkEAEByEAIBAAAMSdAhUIk1gQAASBpCIAAAgCEhBJJqDUIgAACSghAI\nAABgSOqNKMidwSQWhgYAIIkIgQAAAIYk5E6gfDajyUKWEAgAgAQhBAIAABgC51zQIZAUdwMxHQwA\ngOQgBAIAABiCRqujdteFHQKVC3QCAQCQIIRAAAAAQ9APP4IOgUo5dgcDACBBCIEAAACGgBCoNx2s\n2fJdBgAA6CEEAgAAGIL+WjjVgEOgaonpYAAAJAkhEAAAwBD0w4+ZgEOgSjlPCAQAQIIQAgEAAAzB\nMtPBVCnltR51tR51fJcCAAAk5XwXAAAAxlvU6epHL6/oqcWaTp6r6+XldVXLec2WC5otF7RnMq9q\nuaA9kwVVy3ntmYwfL+azvksfqs01gcphh0BSHIil/XwDADAOCIEAAMCOdbtOz19a08nFmk4u1vXU\nYk3Pnl/WRrsrKb7of8uesl64tKYrjZZW1tvXfa1SPqvZ8k8HRNVyQXvKec32wqLZckGzk3GoVC5k\nZWajGu6bUm9Gypg0VQj341Y/BKo3I/3MTNFzNQAAINxPJQCA1Pj8t1/QTCkOEPZOTmjPVEF7J9Pf\naTJszjmdr6/r5Lmanlqs6+RiTU8v1rWyEQc7pXxWdx2o6MPHD+nofFXHDlb0lj3l14Q0UaerWiPS\nlUZLV9Za8ddGpMtrLdUaLV1ei+KvjZZeqjV1ea31umvIFHIZzb6my+ia8KjXdXTTdFE/e/O0shl/\ngVGt2VKllFfGYw2+9UOgGusCAQCQCIRAAICx1mp39an//ey2z00WstozVdCeyQntm4wDg35AtDUs\n6odHpULYodHltdbmlK6TizU9tVjTpdV4e+981nTnzTP6wN37dexgVUfnK3rr3JRy2ddfXjCfzWhu\nekJz0xM7rqPd6arejHRlm/Cof7sfHp16eVm1Rny76177OtPFnI4f3qt7b9+re2/fpztumhppF1G9\n2Q56PSBJqvamwtUbhEAAACQBIRAAYKzls6anfus9urzW0uW1DV1abfVut7S02tLS2oYur7V0ob6u\nZ84va2ltQ1HHbfta5UK2FwgVtHdqYvP2nt79/u34fkHlMZ7ms7rR1g9f6oU95+JpXYtXmpIkM+n2\nuSndf8dcHPgcrOjtt8yMrLMql83E7/fUzoOjbtdpeT3uMLrSiLR4paHvPb+k75xZ0l88+4okad9U\noRcK7dO9t+/Vob3loYZC9WYUfAi0dToYAADwb3w/vQIAIMnMVCnlVSnlddu+yTc83jmnlY22Lq+2\ntLTW0tJqHBItbQZHG1paa+mV5XWdurCspdWWWp3utq9Vymc3A6E9kwVVS3lNFXOamshrupjT1ETv\nv2JO0/2vxbymJnKaLuY0kcuMpDNlo93RcxfihZuf6nX5nL64KtfLwg5USzo2X9GHjh/S0YMV3XWg\nounieIUXmYypWo6nhEnS3zg0qwfuPiBJWrzS0HfPLOm7Z5b07TOX9OcnL0iS9leKuqcXCN371r26\npVLa1ZrqzSjo7eElpoMBAJA0hEAAgKCYmWaKec0U87p1h6HR6kZbl9daW7qMNnoBUmszQLq4sqEz\nF1e1ut7Wynpb7WvnJm0jl7FeaJTbDIbi0Cj/2vvXBElXn4tDp3I+u7nuTKfrdObiqp48V9tcvPnU\nheXN7qe9kwUdPVjR+++6RcfmKzp6sKp9A3TcjKODs2U9uFDWgwvzcs7phUtr+k4vFPrmc6/oK48v\nSpJu2zepe26Pp4/dc3jvQJ1I21luRpqf3d1gadxMF/MyoxMIAICkIAQCAOB1mJmmi3lNF/M6tPeN\nQyMpDo422l2tbrS1ut7W6kYcDK1utLW6EcVB0Zbntt6/tNrSi0uN3vGR1qPtu5BeW2O8A9VUMafl\nZqS1VkeSNDWR088dmNE/ue82HZuPp3UdqJbGZnetYTAzHZ6b0uG5KX3o+CF1u07Pvbyi75y5pO+e\nWdLDT57XFx87K0m68+bpzalj7zq8RzMDdkcxHUzKZuJOvf/26I/1mf9zWrlMRrmMKZu1q7czplzW\nlMvEj2Uzpny293jvfv/5bCaz5bk3vp/LZjZvF/NZTfe68eKvOc0Ur3btvdH6VgAApAEhEAAAu8ws\nvuAs5rNvussm6nS19poQqX1NiPTaUKlcyOrowaqOzVd0eN9U0DtT7UQmYzqyf0ZH9s/on/3CYbU7\nXT39Un2zU+gLj/1En/v2C8qYdNfBam+R6b1aOLTndRcSd86p3ow2F0YO2W//w6N6+qW62l2nTtep\n3XFqd7vx/Y5T1O3Gj/fubz7XdYo6XbW7Xa23+/edOr3n253e6/X+fLTlfvw93rgbb6vSZkh0NSia\n2RIY9ad59p+fuSZQmi7mVcgRJAEAks2cG+wfSAAAhmVhYcGdOHHCdxnApo12R0+crfVCoUt64mxN\n7a5TPmt651tmN3ceu3u++poAYGU90l2f+oY++f479ZH7b/c4gnA559R12gyFmlFHK+ttraxHva/X\nub2x/fPNqPOG37OQy/x0OLQZHuW1b7qgf/7ut97QeMzsB865hRv6wwAA9NAJBAAAcB0TuayOH96r\n44f3Sn/3Dq1ttHXiJ1c2p4/93qM/1u/+5Y9Vyme1cOvs5vSx2d4C1aFPB/PJzJQ1KZvJaiInTU7k\n3lRnXtTpbq75tbwebU7zvBoURb3n4tv95y+urG4GSZVS/oZDIAAAdgMhEAAAwA5NTuT0i3fM6Rfv\nmJMk1RuRHnthaXP62H/82nOSpIleVxAhUHrksxnNThY0O1m44dfoDjhFDQCA3UYIBAAAcIMq5bze\n846b9Z533CxJuriyoe89H4dCp19d0bH5qucKkSSs0QUA8I0QCAAAYJfMTU/oV47t168c2++7FAAA\ngJ/CFgYAAAAAAAABIAQCAAAAAAAIACEQAAAAAABAAAiBAAAAAAAAAkAIBAAAAAAAEABCIAAAAAAA\ngAAQAgEAAAAAAASAEAgAAAAAACAAhEAAAAAAAAABIAQCAAAAAAAIACEQAAAAAABAAAiBAAAAAAAA\nAkAIBAAAAAAAEABCIAAAAAAAgAAQAgEAAAAAAASAEAgAAAAAACAAhEAAAAAAAAABIAQCAAAAAAAI\nACEQAAAAAABAAAiBAAAAAAAAAkAIBAAAAAAAEABCIAAAAAAAgAAQAgEAAAAAAATAnHO+awAAQJJk\nZhcl/eQG//g+SZd2sZxxxfsQ4324ivciNu7vwyHn3JzvIgAA440QCACQCmZ2wjm34LsO33gfYrwP\nV/FexHgfAABgOhgAAAAAAEAQCIEAAAAAAAACQAgEAEiLz/ouICF4H2K8D1fxXsR4HwAAwWNNIAAA\nAAAAgADQCQQAAAAAABAAQiAAwFgzs/ea2Y/M7LSZfcJ3Pb6Y2byZfcvMTpnZM2b2cd81+WRmWTN7\nwsz+3HctvphZ1cy+bGbP9X4u7vFdkw9m9q96vxM/NLM/NrOi75oAAPCFEAgAMLbMLCvp05LeJ+mI\npIfM7IjfqrxpS/pN59zbJR2X9NGA3wtJ+rikU76L8Oz3JH3NOXenpGMK8P0wswOS/qWkBefcz0nK\nSvqg36oAAPCHEAgAMM7eJem0c+5551xL0pckPeC5Ji+ccxecc4/3bq8ovuA/4LcqP8zsoKRflvQH\nvmvxxcxmJN0v6Q8lyTnXcs7V/FblTU5SycxyksqSznuuBwAAbwiBAADj7ICkc1vuLyrQ4GMrM7tV\n0jslPea3Em9+V9K/ltT1XYhHhyVdlPQ/e9Pi/sDMJn0XNWrOuZck/WdJZyVdkFR3zn3Db1UAAPhD\nCAQAGGe2zWNBb3tpZlOSviLpN5xzy77rGTUz+3uSXnXO/cB3LZ7lJP28pM84594paU1ScGtmmdms\n4u7A2yTtlzRpZh/yWxUAAP4QAgEAxtmipPkt9w8q4KkeZpZXHAB9wTn3Vd/1eHKfpA+Y2YuKpwf+\nbTP7X35L8mJR0qJzrt8N9mXFoVBo/o6kF5xzF51zkaSvSrrXc00AAHhDCAQAGGffl/Q2M7vNzAqK\nF3x92HNNXpiZKV7/5ZRz7nd81+OLc+7fOOcOOuduVfzz8E3nXHCdH865lyWdM7Of7T30S5Ke9ViS\nL2clHTezcu935JcU4ALZAAD05XwXAADAjXLOtc3sY5K+rnjXn885557xXJYv90n6sKSnzezJ3mOf\ndM494rEm+PUvJH2hF5A+L+kfe65n5Jxzj5nZlyU9rngHvSckfdZvVQAA+GPOBb10AgAAAAAAQBCY\nDgYAAAAAABAAQiAAAAAAAIAAEAIBAAAAAAAEgBAIAAAAAAAgAIRAAAAAAAAAASAEAgAAAAAACAAh\nEAAAAAAAQAAIgQAAAAAAAALw/wHD8zOJGl67DAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3edecf8e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "code = sess.run(tf.nn.softmax(y_logit), feed_dict={x: batch[0], y_: batch[1]})\n",
    "pred = sess.run(x_gen, feed_dict={x: batch[0], y_: batch[1]})\n",
    "i = 5\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(np.reshape(batch[0][i],(28, 28)))\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(code[i])\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(np.reshape(pred[i],(28, 28)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run((tf.multinomial(y_logit, 1)[:,0], indices), feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.log(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
