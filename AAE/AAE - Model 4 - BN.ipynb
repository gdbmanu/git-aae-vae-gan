{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Obj:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Obj()\n",
    "params.input_dim = 784\n",
    "params.n_l1 = 1000\n",
    "params.n_l2 = 1000\n",
    "params.n_labels = 10\n",
    "params.z_dim = 15\n",
    "params.batch_size = 100\n",
    "params.n_epochs = 1000\n",
    "params.learning_rate = 0.001\n",
    "params.beta1 = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = './Results/Autoencoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(x, n1, n2, name, stddev = .1):\n",
    "    \"\"\"\n",
    "    Used to create a dense layer.\n",
    "    :param x: input tensor to the dense layer\n",
    "    :param n1: no. of input neurons\n",
    "    :param n2: no. of output neurons\n",
    "    :param name: name of the entire dense layer.i.e, variable scope name.\n",
    "    :return: tensor with shape [batch_size, n2]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name, reuse=None):\n",
    "        #xav_stddev = np.sqrt(2 / (n1 + n2))\n",
    "        xav_stddev = stddev * np.sqrt( 1. / n1)\n",
    "        #xav_stddev_b = np.sqrt(2 / n2)\n",
    "        xav_stddev_b = stddev\n",
    "        weights = tf.get_variable(\"weights\", shape=[n1, n2],\n",
    "                                  initializer=tf.random_normal_initializer(mean=0., stddev=xav_stddev))\n",
    "        #bias = tf.get_variable(\"bias\", shape=[n2], initializer=tf.constant_initializer(0.0))\n",
    "        bias = tf.get_variable(\"bias\", shape=[n2], initializer=tf.random_normal_initializer(mean=0., stddev=xav_stddev_b))\n",
    "        matmul = tf.add(tf.matmul(x, weights), bias, name='matmul')\n",
    "    return matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, params, reuse=False, batch_phase = True):\n",
    "    \"\"\"\n",
    "    Encode part of the autoencoder\n",
    "    :param x: input to the autoencoder\n",
    "    :param reuse: True -> Reuse the encoder variables, False -> Create or search of variables before creating\n",
    "    :return: tensor which is the hidden latent variable of the autoencoder.\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Encoder'):\n",
    "        e_h1 = dense(x, params.input_dim, params.n_l1, 'e_h1')\n",
    "        e_h1_bn = tf.contrib.layers.batch_norm(e_h1, center=True, scale=True, is_training=batch_phase, scope='e_h1')\n",
    "        e_dense_1 = tf.nn.relu(e_h1_bn)\n",
    "        \n",
    "        e_h2 = dense(e_dense_1, params.n_l1, params.n_l2, 'e_h2')\n",
    "        e_h2_bn = tf.contrib.layers.batch_norm(e_h2, center=True, scale=True, is_training=batch_phase, scope='e_h2')\n",
    "        e_dense_2 = tf.nn.relu(e_h2_bn)\n",
    "        \n",
    "        e_latent_variable = dense(e_dense_2, params.n_l2, params.z_dim, 'e_latent_variable')\n",
    "        e_label = dense(e_dense_2, params.n_l2, params.n_labels, 'e_label')\n",
    "    return e_label, e_latent_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x, params, reuse=False, batch_phase = True):\n",
    "    \"\"\"\n",
    "    Decoder part of the autoencoder\n",
    "    :param x: input to the decoder\n",
    "    :param reuse: True -> Reuse the decoder variables, False -> Create or search of variables before creating\n",
    "    :return: tensor which should ideally be the input given to the encoder.\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Decoder'):\n",
    "        #d_dense_1 = tf.nn.relu(dense(x, params.n_labels + params.z_dim, params.n_l2, 'd_dense_1'))\n",
    "        #d_dense_2 = tf.nn.relu(dense(d_dense_1, params.n_l2, params.n_l1, 'd_dense_2'))\n",
    "        #output = tf.nn.sigmoid(dense(d_dense_2, params.n_l1, params.input_dim, 'd_output'))\n",
    "        \n",
    "        d_h1 = dense(x, params.n_labels + params.z_dim,  params.n_l2, 'd_h1')\n",
    "        d_h1_bn = tf.contrib.layers.batch_norm(d_h1, center=True, scale=True, is_training=batch_phase, scope='d_h1')\n",
    "        d_dense_1 = tf.nn.relu(d_h1_bn)\n",
    "        \n",
    "        d_h2 = dense(d_dense_1, params.n_l2, params.n_l1, 'd_h2')\n",
    "        d_h2_bn = tf.contrib.layers.batch_norm(d_h2, center=True, scale=True, is_training=batch_phase, scope='d_h2')\n",
    "        d_dense_2 = tf.nn.relu(d_h2_bn)\n",
    "        \n",
    "        d_output = dense(d_dense_2, params.n_l1, params.input_dim, 'd_output')\n",
    "        return d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_gauss(x, params, reuse=False, batch_phase = True):\n",
    "    \"\"\"\n",
    "    Discriminator that is used to match the posterior distribution with a given prior distribution.\n",
    "    :param x: tensor of shape [batch_size, z_dim]\n",
    "    :param reuse: True -> Reuse the discriminator variables,\n",
    "                  False -> Create or search of variables before creating\n",
    "    :return: tensor of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Discriminator_Gauss'):\n",
    "        #dc_g_dense_1 = tf.nn.relu(dense(x, params.z_dim, params.n_l1, name='dc_g_dense_1'))\n",
    "        #dc_g_dense_2 = tf.nn.relu(dense(dc_g_dense_1, params.n_l1, params.n_l2, name='dc_g_dense_2'))\n",
    "        \n",
    "        dc_g_h1 = dense(x, params.z_dim, params.n_l1, 'dc_g_h1')\n",
    "        dc_g_h1_bn = tf.contrib.layers.batch_norm(dc_g_h1, center=True, scale=True, is_training=batch_phase, scope='dc_g_h1')\n",
    "        dc_g_dense_1 = tf.nn.relu(dc_g_h1_bn)\n",
    "        \n",
    "        dc_g_h2 = dense(dc_g_dense_1, params.n_l1, params.n_l2, 'dc_g_h2')\n",
    "        dc_g_h2_bn = tf.contrib.layers.batch_norm(dc_g_h2, center=True, scale=True, is_training=batch_phase, scope='dc_g_h2')\n",
    "        dc_g_dense_2 = tf.nn.relu(dc_g_h2_bn)\n",
    "        \n",
    "        dc_g_output = dense(dc_g_dense_2, params.n_l2, 1, name='dc_g_output')\n",
    "    return dc_g_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_categorical(x, params, reuse=False, batch_phase = True):\n",
    "    \"\"\"\n",
    "    Discriminator that is used to match the posterior distribution with a given prior distribution.\n",
    "    :param x: tensor of shape [batch_size, z_dim]\n",
    "    :param reuse: True -> Reuse the discriminator variables,\n",
    "                  False -> Create or search of variables before creating\n",
    "    :return: tensor of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Discriminator_Categorical'):\n",
    "        #dc_c_dense_1 = tf.nn.relu(dense(x, params.n_labels, params.n_l1, name='dc_c_dense_1'))\n",
    "        #dc_c_dense_2 = tf.nn.relu(dense(dc_c_dense_1, params.n_l1, params.n_l2, name='dc_c_dense_2'))\n",
    "        dc_c_h1 = dense(x, params.n_labels, params.n_l1, 'dc_c_h1')\n",
    "        dc_c_h1_bn = tf.contrib.layers.batch_norm(dc_c_h1, center=True, scale=True, is_training=batch_phase, scope='dc_c_h1')\n",
    "        dc_c_dense_1 = tf.nn.relu(dc_c_h1_bn)\n",
    "        \n",
    "        dc_c_h2 = dense(dc_c_dense_1, params.n_l1, params.n_l2, 'dc_c_h2')\n",
    "        dc_c_h2_bn = tf.contrib.layers.batch_norm(dc_c_h2, center=True, scale=True, is_training=batch_phase, scope='dc_c_h2')\n",
    "        dc_c_dense_2 = tf.nn.relu(dc_c_h2_bn)\n",
    "        \n",
    "        dc_c_output = dense(dc_c_dense_2, params.n_l2, 1, name='dc_c_output')\n",
    "    return dc_c_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = tf.placeholder(dtype=tf.float32, shape=[params.batch_size, params.input_dim], name='Input')\n",
    "x_input_l = tf.placeholder(dtype=tf.float32, shape=[params.batch_size, params.input_dim], name='Labeled_Input')\n",
    "x_target = tf.placeholder(dtype=tf.float32, shape=[params.batch_size, params.input_dim], name='Target')\n",
    "y_label = tf.placeholder(dtype=tf.float32, shape=[params.batch_size, params.n_labels], name='Labels')\n",
    "real_distribution = tf.placeholder(dtype=tf.float32, shape=[params.batch_size, params.z_dim], name='Real_distribution')\n",
    "categorical_distribution = tf.placeholder(dtype=tf.float32, shape=[params.batch_size, params.n_labels], name='Categorical_distribution')\n",
    "decoder_input = tf.placeholder(dtype=tf.float32, shape=[1, params.n_labels + params.z_dim], name='Decoder_input')\n",
    "bn_phase = tf.placeholder(tf.bool, name='bn_phase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    e_label_logit, e_latent_variable  = encoder(x_input, params, batch_phase = bn_phase)\n",
    "    #e_softmax_label = tf.nn.softmax(logits=e_label_logit, name='e_softmax_label')\n",
    "    e_softmax_label = tf.nn.sigmoid(e_label_logit, name='e_softmax_label')\n",
    "    d_output = decoder(tf.concat([e_softmax_label, e_latent_variable], 1), params, batch_phase = bn_phase) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    d_g_real = discriminator_gauss(real_distribution, params, batch_phase = bn_phase)\n",
    "    d_g_fake = discriminator_gauss(e_latent_variable, params, reuse=True, batch_phase = bn_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    d_c_real = discriminator_categorical(categorical_distribution, params, batch_phase = bn_phase)\n",
    "    d_c_fake = discriminator_categorical(e_softmax_label, params, reuse=True, batch_phase = bn_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\" phase 3 (classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    e_label_logit_l, _ = encoder(x_input_l, params, reuse=True, batch_phase = bn_phase)\n",
    "    #e_softmax_label_l = tf.nn.softmax(logits=e_label_logit_l, name='e_softmax_label_l')\n",
    "    e_softmax_label_l = tf.nn.sigmoid(e_label_logit_l, name='e_softmax_label_l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    decoder_image = decoder(decoder_input, params, reuse=True, batch_phase = bn_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 autoencoder Loss\n",
    "\n",
    "autoencoder_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_output, labels=x_target), reduction_indices=1)\n",
    "\n",
    "# Phase 2 Discriminator Loss\n",
    "\n",
    "# Gaussian Discriminator Loss\n",
    "dc_g_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_g_real), logits=d_g_real))\n",
    "dc_g_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_g_fake), logits=d_g_fake))\n",
    "dc_g_loss = dc_g_loss_fake + dc_g_loss_real\n",
    "\n",
    "# Categorical Discrimminator Loss\n",
    "dc_c_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_c_real), logits=d_c_real))\n",
    "dc_c_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_c_fake), logits=d_c_fake))\n",
    "dc_c_loss = dc_c_loss_fake + dc_c_loss_real\n",
    "\n",
    "# Generator loss\n",
    "generator_g_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_g_real), logits=d_g_real))\n",
    "generator_g_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_g_fake), logits=d_g_fake))\n",
    "generator_g_loss = generator_g_loss_fake + generator_g_loss_real\n",
    "\n",
    "generator_c_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_c_real), logits=d_c_real))\n",
    "generator_c_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_c_fake), logits=d_c_fake))\n",
    "generator_c_loss = generator_c_loss_fake + generator_c_loss_real\n",
    "generator_loss = generator_c_loss + generator_g_loss\n",
    "\n",
    "# Phase 3 : classifier loss\n",
    "\n",
    "#supervised_encoder_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_label, logits=e_label_logit_l))\n",
    "supervised_encoder_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_label, logits=e_label_logit_l), reduction_indices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variables = tf.trainable_variables()\n",
    "dc_g_var = [var for var in all_variables if 'dc_g_' in var.name]\n",
    "dc_c_var = [var for var in all_variables if 'dc_c_' in var.name]\n",
    "en_var = [var for var in all_variables if 'e_' in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'e_h1/weights:0' shape=(784, 1000) dtype=float32_ref>\", \"<tf.Variable 'e_h1/bias:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'e_h1/beta:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'e_h1/gamma:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'e_h2/weights:0' shape=(1000, 1000) dtype=float32_ref>\", \"<tf.Variable 'e_h2/bias:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'e_h2/beta:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'e_h2/gamma:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'e_latent_variable/weights:0' shape=(1000, 15) dtype=float32_ref>\", \"<tf.Variable 'e_latent_variable/bias:0' shape=(15,) dtype=float32_ref>\", \"<tf.Variable 'e_label/weights:0' shape=(1000, 10) dtype=float32_ref>\", \"<tf.Variable 'e_label/bias:0' shape=(10,) dtype=float32_ref>\"] and loss Tensor(\"Mean_4:0\", shape=(), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c16fb4bc85b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdiscriminator_optimizer_real_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m                                                        \u001b[0mbeta1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdc_g_loss_real\u001b[0m\u001b[0;34m,\u001b[0m                                                                                     \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdc_g_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdiscriminator_optimizer_fake_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m                                                        \u001b[0mbeta1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdc_g_loss_fake\u001b[0m\u001b[0;34m,\u001b[0m                                                                                      \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdc_g_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgenerator_optimizer_real_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m                                                    \u001b[0mbeta1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_g_loss_real\u001b[0m\u001b[0;34m,\u001b[0m                                                                                  \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0men_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mgenerator_optimizer_fake_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m                                                    \u001b[0mbeta1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_g_loss_fake\u001b[0m\u001b[0;34m,\u001b[0m                                                                                  \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0men_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    348\u001b[0m           \u001b[0;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m           \u001b[0;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'e_h1/weights:0' shape=(784, 1000) dtype=float32_ref>\", \"<tf.Variable 'e_h1/bias:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'e_h1/beta:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'e_h1/gamma:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'e_h2/weights:0' shape=(1000, 1000) dtype=float32_ref>\", \"<tf.Variable 'e_h2/bias:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'e_h2/beta:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'e_h2/gamma:0' shape=(1000,) dtype=float32_ref>\", \"<tf.Variable 'e_latent_variable/weights:0' shape=(1000, 15) dtype=float32_ref>\", \"<tf.Variable 'e_latent_variable/bias:0' shape=(15,) dtype=float32_ref>\", \"<tf.Variable 'e_label/weights:0' shape=(1000, 10) dtype=float32_ref>\", \"<tf.Variable 'e_label/bias:0' shape=(10,) dtype=float32_ref>\"] and loss Tensor(\"Mean_4:0\", shape=(), dtype=float32)."
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "\n",
    "# Phase 1\n",
    "decoder_optimizer = tf.train.AdamOptimizer(learning_rate=params.learning_rate,\\\n",
    "                                           beta1=params.beta1).minimize(autoencoder_loss)\n",
    "\n",
    "# Phase 2\n",
    "discriminator_optimizer_real_g = tf.train.AdamOptimizer(learning_rate=params.learning_rate,\\\n",
    "                                                        beta1=params.beta1).minimize(dc_g_loss_real,\\\n",
    "                                                                                     var_list=dc_g_var)\n",
    "discriminator_optimizer_fake_g = tf.train.AdamOptimizer(learning_rate=params.learning_rate,\\\n",
    "                                                        beta1=params.beta1).minimize(dc_g_loss_fake, \\\n",
    "                                                                                     var_list=dc_g_var)\n",
    "generator_optimizer_real_g = tf.train.AdamOptimizer(learning_rate=params.learning_rate,\\\n",
    "                                                    beta1=params.beta1).minimize(generator_g_loss_real, \\\n",
    "                                                                                 var_list=en_var)\n",
    "generator_optimizer_fake_g = tf.train.AdamOptimizer(learning_rate=params.learning_rate,\\\n",
    "                                                    beta1=params.beta1).minimize(generator_g_loss_fake, \\\n",
    "                                                                                 var_list=en_var)\n",
    "\n",
    "discriminator_optimizer_real_c = tf.train.AdamOptimizer(learning_rate=params.learning_rate,\\\n",
    "                                                        beta1=params.beta1).minimize(dc_c_loss_real, \\\n",
    "                                                                                     var_list=dc_c_var)\n",
    "discriminator_optimizer_fake_c = tf.train.AdamOptimizer(learning_rate=params.learning_rate,\\\n",
    "                                                        beta1=params.beta1).minimize(dc_c_loss_fake, \\\n",
    "                                                                                     var_list=dc_c_var)\n",
    "generator_optimizer_real_c = tf.train.AdamOptimizer(learning_rate=params.learning_rate,\\\n",
    "                                                    beta1=params.beta1).minimize(generator_c_loss_real, \\\n",
    "                                                                                 var_list=en_var)\n",
    "generator_optimizer_fake_c = tf.train.AdamOptimizer(learning_rate=params.learning_rate,\\\n",
    "                                                    beta1=params.beta1).minimize(generator_c_loss_fake, \\\n",
    "                                                                                 var_list=en_var)\n",
    "# Phase 3\n",
    "supervised_encoder_optimizer = tf.train.AdamOptimizer(learning_rate=params.learning_rate, \\\n",
    "                                                      beta1=params.beta1).minimize(supervised_encoder_loss, \\\n",
    "                                                                            var_list=en_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "#gen_accuracy = tf.sqrt(tf.reduce_mean(tf.square(x_target - tf.nn.sigmoid(decoder_output))))\n",
    "correct_prediction = tf.equal(tf.argmax(e_label_logit, 1), tf.argmax(y_label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "gen_accuracy = tf.sqrt(tf.reduce_mean(tf.square(x_target - tf.nn.sigmoid(d_output))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn = 1\n",
    "if turn ==1:\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saver.restore(sess, \"./AAE_Model4full_fullBase_alpha_1em4_beta1_9em1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(params):\n",
    "    if FLAG_SEMI:\n",
    "        indices = np.arange(TEST_SIZE)\n",
    "        np.random.shuffle(indices)\n",
    "        batch_x, batch_y = subset_x[indices[:params.batch_size]], subset_y[indices[:params.batch_size]]\n",
    "    else:\n",
    "        batch_x, batch_y = mnist.train.next_batch(params.batch_size)\n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 1000\n",
    "FLAG_SEMI = False\n",
    "FLAG_DECODE = True\n",
    "FLAG_CLASSIF = True\n",
    "FLAG_GAN = True\n",
    "\n",
    "#params.learning_rate = 0.0001\n",
    "#params.beta1 = 0.5\n",
    "\n",
    "mem = Obj()\n",
    "mem.num_epoch = []\n",
    "mem.classif_eval = []\n",
    "mem.decoder_eval = []\n",
    "mem.dc_g_eval = []\n",
    "mem.gen_g_eval =  []\n",
    "mem.dc_c_eval = []\n",
    "mem.gen_c_eval =  []\n",
    "\n",
    "if FLAG_SEMI :\n",
    "    subset_x, subset_y =  mnist.train.next_batch(TEST_SIZE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.n_epochs = 10000\n",
    "\n",
    "for num_epoch in range (params.n_epochs):\n",
    "    \n",
    "    if num_epoch % 10 == 0:\n",
    "        mem.num_epoch += [num_epoch]\n",
    "        \n",
    "        \n",
    "        if FLAG_CLASSIF:\n",
    "            x_test, y_test = mnist.test.next_batch(params.batch_size)\n",
    "            classif_eval = accuracy.eval(feed_dict={x_input: x_test, y_label: y_test, batch_phase:False})\n",
    "            mem.classif_eval += [classif_eval]\n",
    "        else:\n",
    "            classif_eval = 0\n",
    "        \n",
    "        x_test, y_test = next_batch(params)\n",
    "        if FLAG_DECODE:\n",
    "            decoder_eval = gen_accuracy.eval(feed_dict={x_input: x_test, y_label: y_test, x_target: x_test, batch_phase:False})\n",
    "            mem.decoder_eval += [decoder_eval]\n",
    "        else:\n",
    "            decoder_eval = 0   \n",
    "        \n",
    "        if FLAG_GAN :\n",
    "            z_real_dist = np.random.randn(params.batch_size, params.z_dim) \n",
    "            dc_g_eval = dc_g_loss.eval(feed_dict={x_input: x_test, \\\n",
    "                                                  real_distribution: z_real_dist,\\\n",
    "                                                  batch_phase:False})\n",
    "            gen_g_eval = generator_g_loss.eval(feed_dict={x_input: x_test, \\\n",
    "                                                          real_distribution: z_real_dist,\\\n",
    "                                                          batch_phase:False})\n",
    "            mem.dc_g_eval += [dc_g_eval]\n",
    "            mem.gen_g_eval += [gen_g_eval]\n",
    "            \n",
    "            c_real_dist = np.random.randint(low=0, high=params.n_labels, size=params.batch_size)\n",
    "            c_real_dist = np.eye(params.n_labels)[c_real_dist]\n",
    "            dc_c_eval = dc_c_loss.eval(feed_dict={x_input: x_test, \\\n",
    "                                                  categorical_distribution: c_real_dist,\\\n",
    "                                                  batch_phase:False})\n",
    "            gen_c_eval = generator_c_loss.eval(feed_dict={x_input: x_test, \\\n",
    "                                                      categorical_distribution: c_real_dist,\\\n",
    "                                                      batch_phase:False})\n",
    "            mem.dc_c_eval += [dc_c_eval]\n",
    "            mem.gen_c_eval += [gen_c_eval]        \n",
    "        else:\n",
    "            dc_g_eval = 0\n",
    "            gen_g_eval = 0\n",
    "            dc_c_eval = 0\n",
    "            gen_c_eval = 0\n",
    "            \n",
    "        sys.stdout.write('\\rstep %d\\t dec : %.5f\\t classif : %.5f\\tdiscr_g : %.5f\\t gen_g : %.5f, discr_c : %.5f\\t gen_c : %.5f' \\\n",
    "                         % (num_epoch, \\\n",
    "                            decoder_eval, \\\n",
    "                            classif_eval,\\\n",
    "                            dc_g_eval, \\\n",
    "                            gen_g_eval, \\\n",
    "                            dc_c_eval, \\\n",
    "                            gen_c_eval))\n",
    "    for _ in range(2):\n",
    "        if FLAG_DECODE:\n",
    "            batch_x, batch_y = mnist.train.next_batch(params.batch_size) #next_batch(params)\n",
    "            decoder_optimizer.run(feed_dict={x_input: batch_x, x_target: batch_x, batch_phase:True})\n",
    "\n",
    "        if FLAG_CLASSIF:\n",
    "            batch_x, batch_y = next_batch(params)\n",
    "            supervised_encoder_optimizer.run(feed_dict={x_input_l: batch_x, y_label: batch_y, batch_phase:True})\n",
    "    \n",
    "    if FLAG_GAN :\n",
    "        z_real_dist = np.random.randn(params.batch_size, params.z_dim) \n",
    "        discriminator_optimizer_real_g.run(feed_dict={real_distribution: z_real_dist, batch_phase:True})\n",
    "        z_real_dist = np.random.randn(params.batch_size, params.z_dim) \n",
    "        generator_optimizer_real_g.run(feed_dict={real_distribution: z_real_dist, batch_phase:True})\n",
    "        \n",
    "        c_real_dist = np.random.randint(low=0, high=params.n_labels, size=params.batch_size)\n",
    "        c_real_dist = np.eye(params.n_labels)[c_real_dist]\n",
    "        discriminator_optimizer_real_c.run(feed_dict={categorical_distribution: c_real_dist, batch_phase:True})\n",
    "        c_real_dist = np.random.randint(low=0, high=params.n_labels, size=params.batch_size)\n",
    "        c_real_dist = np.eye(params.n_labels)[c_real_dist]\n",
    "        generator_optimizer_real_c.run(feed_dict={categorical_distribution: c_real_dist, batch_phase:True})        \n",
    "        \n",
    "        \n",
    "        #batch_x, batch_y = next_batch(params)\n",
    "        #generator_optimizer_fake.run(feed_dict={x_input: batch_x})\n",
    "    \n",
    "    for _ in range(2):\n",
    "        if FLAG_DECODE:\n",
    "            batch_x, batch_y = mnist.train.next_batch(params.batch_size) #next_batch(params)\n",
    "            decoder_optimizer.run(feed_dict={x_input: batch_x, x_target: batch_x, batch_phase:True})\n",
    "\n",
    "        if FLAG_CLASSIF:\n",
    "            batch_x, batch_y = next_batch(params)\n",
    "            supervised_encoder_optimizer.run(feed_dict={x_input_l: batch_x, y_label: batch_y, batch_phase:True})\n",
    "        \n",
    "    if FLAG_GAN :    \n",
    "        batch_x, batch_y = mnist.train.next_batch(params.batch_size) #next_batch(params)        \n",
    "        discriminator_optimizer_fake_g.run(feed_dict={x_input: batch_x, batch_phase:True})\n",
    "        batch_x, batch_y = mnist.train.next_batch(params.batch_size) #next_batch(params)\n",
    "        generator_optimizer_fake_g.run(feed_dict={x_input: batch_x, batch_phase:True})\n",
    "        \n",
    "        batch_x, batch_y = mnist.train.next_batch(params.batch_size) #next_batch(params)        \n",
    "        discriminator_optimizer_fake_c.run(feed_dict={x_input: batch_x, batch_phase:True})\n",
    "        batch_x, batch_y = mnist.train.next_batch(params.batch_size) #next_batch(params)\n",
    "        generator_optimizer_fake_c.run(feed_dict={x_input: batch_x, batch_phase:True})\n",
    "        \n",
    "    for _ in range(0):\n",
    "        if FLAG_DECODE:\n",
    "            batch_x, batch_y = mnist.train.next_batch(params.batch_size) #next_batch(params)\n",
    "            decoder_optimizer.run(feed_dict={x_input: batch_x, x_target: batch_x, batch_phase:True})\n",
    "\n",
    "        if FLAG_CLASSIF:\n",
    "            batch_x, batch_y = next_batch(params)\n",
    "            supervised_encoder_optimizer.run(feed_dict={x_input_l: batch_x, y_label: batch_y, batch_phase:True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saver.save(sess, \"./AAE_Model4full_fullBase_alpha_1em6_beta1_9em1_1gR0F1c.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "plt.subplot(1,4,1)\n",
    "plt.plot(mem.decoder_eval)\n",
    "plt.legend(('reconstruction err',))\n",
    "plt.subplot(1,4,2)\n",
    "plt.plot( mem.classif_eval)\n",
    "plt.legend(('classif rate',))\n",
    "plt.subplot(1,4,3)\n",
    "plt.plot( mem.dc_g_eval)\n",
    "plt.plot( mem.gen_g_eval)\n",
    "plt.legend(('dc_g','gen_g'))\n",
    "plt.subplot(1,4,4)\n",
    "plt.plot( mem.dc_c_eval)\n",
    "plt.plot( mem.gen_c_eval)\n",
    "plt.legend(('dc_c','gen_c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = mnist.test.next_batch(params.batch_size) #.images, mnist.test.labels #\n",
    "print(accuracy.eval(feed_dict={x_input: x_test, y_label: y_test, batch_phase:False}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(accuracy.eval(feed_dict={x_input:subset_x[np.arange(params.batch_size)], y:subset_y[np.arange(params.batch_size)]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indices[:params.batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_z = e_latent_variable.eval(feed_dict={x_input: batch_x, batch_phase:False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_c = e_label_logit.eval(feed_dict={x_input: batch_x, batch_phase:False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(liste_z[:,0], liste_z[:,1], '.')\n",
    "plt.plot(z_real_dist[:,0], z_real_dist[:,1], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(liste_z[:,0], liste_z[:,1], '.')\n",
    "plt.plot(z_real_dist[:,0], z_real_dist[:,1], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10, 15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(batch_y, aspect = 'auto')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(1/(1+np.exp(-liste_c)), aspect = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste_z = e_latent_variable.eval(feed_dict={x_input: batch_x})\n",
    "z_cat = {}\n",
    "for i in range(10):\n",
    "    z_cat[i] = []\n",
    "for i in range(len(liste_z)):\n",
    "    index = np.where(batch_y[i] == 1)[0][0]\n",
    "    z_cat[index] += [liste_z[i]]\n",
    "for i in range(10):\n",
    "    z_aff = np.transpose(z_cat[i])\n",
    "    plt.plot(z_aff[0,:], z_aff[1,:], '.')\n",
    "plt.legend((0,1,2,3,4,5,6,7,8,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste_z = e_latent_variable.eval(feed_dict={x_input: x_test})\n",
    "z_cat = {}\n",
    "for i in range(10):\n",
    "    z_cat[i] = []\n",
    "for i in range(len(liste_z)):\n",
    "    index = np.where(y_test[i] == 1)[0][0]\n",
    "    z_cat[index] += [liste_z[i]]\n",
    "for i in range(10):\n",
    "    z_aff = np.transpose(z_cat[i])\n",
    "    plt.plot(z_aff[0,:], z_aff[1,:], '.')\n",
    "plt.legend((0,1,2,3,4,5,6,7,8,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = tf.nn.sigmoid(d_output).eval(feed_dict={x_input: batch_x, batch_phase:False})\n",
    "i = 1\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(batch_x[i].reshape((28, 28)), cmap = 'gray_r')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(x_out[i].reshape((28, 28)), cmap = 'gray_r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "cat_test = 4\n",
    "cat = np.where(liste_c[i] == np.max(liste_c[i]))[0][0]\n",
    "print(cat)\n",
    "style = liste_z[i]\n",
    "print(style)\n",
    "\n",
    "decoder_feed = np.zeros(params.n_labels + params.z_dim)\n",
    "# label\n",
    "decoder_feed[cat_test] = 1\n",
    "#np.concat(np.random.randn((params.z_dim)).reshape((1, params.z_dim))\n",
    "# style\n",
    "decoder_feed[params.n_labels:params.n_labels + params.z_dim] = style\n",
    "decoder_feed = decoder_feed.reshape((1, params.n_labels + params.z_dim))\n",
    "\n",
    "x_image = tf.nn.sigmoid(decoder_image).eval(feed_dict={decoder_input : decoder_feed, batch_phase:False})\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(x_image.reshape((28, 28)), cmap = 'gray_r')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(x_image.reshape((28, 28)), cmap = 'gray_r')\n",
    "#plt.imshow(x_image.reshape((28, 28)), cmap = 'gray_r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "decoder_feed = np.zeros(params.n_labels + params.z_dim)\n",
    "# label\n",
    "cat = sigmoid(liste_c[i])\n",
    "decoder_feed[:params.n_labels] = cat\n",
    "style = liste_z[i]\n",
    "decoder_feed[params.n_labels:] = style\n",
    "decoder_feed = decoder_feed.reshape((1, params.n_labels + params.z_dim))\n",
    "\n",
    "x_image = tf.nn.sigmoid(decoder_image).eval(feed_dict={decoder_input : decoder_feed, batch_phase:False})\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(x_image.reshape((28, 28)), cmap = 'gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
